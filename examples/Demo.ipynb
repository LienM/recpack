{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "prepared-afghanistan",
   "metadata": {},
   "source": [
    "# Demo Recpack\n",
    "\n",
    "This is an end to end demo of how to set up an experimental pipeline with Recpack.\n",
    "\n",
    "It covers:\n",
    "- Loading a dataset\n",
    "- Preprocessing the dataset\n",
    "- Transforming it to the appropriate data format\n",
    "- Splitting the dataset into a training(, validation) and test set. \n",
    "- Training the algorithm(s)\n",
    "- Making predictions\n",
    "- Evaluating the performance of the algorithm(s) using metrics\n",
    "\n",
    "### Dataset\n",
    "\n",
    "Recpack provides a set of datasets you can use out of the box. These are:\n",
    "- MovieLens 25M\n",
    "- RecSys Challenge 2015\n",
    "- CiteULike\n",
    "\n",
    "If your dataset of choice is not included in this list, you can choose to either:\n",
    "- Create your own Dataset child class for this dataset.\n",
    "- Perform the transformation to an InteractionMatrix yourself.\n",
    "\n",
    "In this example we use the MovieLens 25M dataset which contains user-item rating tuples and timestamp information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liberal-cocktail",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from recpack.data.datasets import MovieLens25M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-business",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('data'):\n",
    "    os.mkdir('data')\n",
    "# We set preprocess_default to false, so we can highlight the selection and application of filters\n",
    "dataset = MovieLens25M(\"data/ml25.csv\", preprocess_default=False)\n",
    "# Download the dataset if not present at \"data/ml25.csv\"\n",
    "dataset.fetch_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artistic-lobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all interactions as a pandas DataFrame\n",
    "df = dataset.load_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-subscriber",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-tablet",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "Recpack makes it easy to perform some classic preprocessing steps.\n",
    "When using a Dataset, the preprocessing steps are performed when `load_interaction_matrix()` is called.\n",
    "\n",
    "Every Dataset comes with a set of default filters, i.e. typical preprocessing steps applied to this dataset in the literature.\n",
    "To overwrite these add the `preprocess_default=False` keyword argument when constructing the object, then proceed to define your own filters.\n",
    "\n",
    "In this example we won't use the default ML25M filters, but instead define our own. \n",
    "\n",
    "* MinRating = 3 -> Any rating of 3 and above is considered a positive interaction.\n",
    "* MinUsersPerItem = 30 -> Remove all items that fewer than 30 people interacted with, otherwise computation might go out of RAM.\n",
    "* MinItemsPerUser = 5 -> Remove all users who interacted with less than 5 items.\n",
    "\n",
    "The order in which these are added is important, as they are applied in order:\n",
    "first the rating filter will be applied, then the users per item and finally items per user.\n",
    "\n",
    "We usually apply the strictest filter first. \n",
    "Otherwise we might count interactions with rating < 3 in our min user per item filter, but then throw them away again later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spanish-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recpack.preprocessing.filters import MinItemsPerUser, MinRating, MinUsersPerItem, NMostPopular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-combination",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.add_filter(MinRating(3, rating_ix=\"rating\"))\n",
    "dataset.add_filter(MinUsersPerItem(10, item_ix=\"movieId\", user_ix=\"userId\", count_duplicates=False))\n",
    "dataset.add_filter(MinItemsPerUser(5, item_ix=\"movieId\", user_ix=\"userId\", count_duplicates=False))\n",
    "dataset.add_filter(NMostPopular(1000, item_ix=\"movieId\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understood-chile",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies filters, and loads filtered data into an InteractionMatrix\n",
    "data = dataset.load_interaction_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-republican",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_users = df.userId.nunique()\n",
    "original_items = df.movieId.nunique()\n",
    "users, items = data.shape\n",
    "\n",
    "print(f\"We have {users} users, {items} items and {data.num_interactions} interactions left\")\n",
    "print(f\"preprocessing removed {original_users - users} users\")\n",
    "print(f\"preprocessing removed {original_items - items} items\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naked-innocent",
   "metadata": {},
   "source": [
    "### Scenario\n",
    "\n",
    "A scenario describes a situation in which to evaluate the performance of our algorithm.\n",
    "Examples are predicting for users that were not in the training set (StrongGeneralization),\n",
    "or predicting future interactions of the users in the training dataset (Timed).\n",
    "\n",
    "We're not optimizing any hyperparameters here, so we split into only a training and test set.\n",
    "If you do wish to optimize hyperparameters, add `validation=True` to the constructor of your scenario,\n",
    "and use this validation data to determine the optimal hyperparameters.\n",
    "\n",
    "Here we choose the StrongGeneralization scenario. This means we won't use the timestamp information for now.\n",
    "StrongGeneralization splits users into two datasets, only the training users' interactions are used for training.\n",
    "Test users' interactions are again split in two: one part is used as history to base predictions on,\n",
    "the other part consists of the held-out interactions we will try to predict.\n",
    "\n",
    "As parameters this scenario allows you to select the fraction of users to use for training, \n",
    "and the fraction of user interactions to use as history for your predictions.\n",
    "\n",
    "We will use 70% of users as training data.\n",
    "For prediction we will use 80% of the test users' interactions as history and predict the remaining 20%.\n",
    "Calidation is set to False because we do not require validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-trademark",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recpack.splitters.scenarios import StrongGeneralization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bored-container",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = StrongGeneralization(frac_users_train=0.7, frac_interactions_in=0.8, validation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-balance",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario.split(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colonial-cincinnati",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "We'll use a pipeline to do the heavy lifting for us.\n",
    "\n",
    "We construct the pipeline using the pipeline builder. In the next steps we will add the data, select the algorithms, set the metrics, and finally run the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genetic-jackson",
   "metadata": {},
   "outputs": [],
   "source": [
    "import recpack.pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungry-hormone",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_builder = recpack.pipelines.PipelineBuilder('demo')\n",
    "pipeline_builder.set_train_data(scenario.training_data)\n",
    "pipeline_builder.set_test_data(scenario.test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-summit",
   "metadata": {},
   "source": [
    "### Algorithms\n",
    "\n",
    "We now choose two algorithms to evaluate:\n",
    "\n",
    "* Item KNN\n",
    "* Popularity \n",
    "\n",
    "You can also add EASE, but make sure to have enough RAM available, at least 32GB needed. \n",
    "\n",
    "Each algorithm has a set of parameters, so in practical settings, you would optimise them before comparison. \n",
    "Here we don't care as much about optimality, and so we just picked defaults that made some sense.\n",
    "\n",
    "It is also entirely possible to create your own algorithm. To learn how to do so, check out the Getting Started guide on docs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-testing",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_builder.add_algorithm('ItemKNN', params={'K': 200})\n",
    "# pipeline_builder.add_algorithm('ItemKNN', params={'K': 300})\n",
    "pipeline_builder.add_algorithm('Popularity', params={'K': 50})\n",
    "# pipeline_builder.add_algorithm('EASE', params={'l2': 100})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continental-breeding",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "We now select a couple metrics that will be evaluated on\n",
    " \n",
    "* CoverageK\n",
    "* CalibratedRecallK\n",
    "* NormalizedDiscountedCumulativeGainK\n",
    "* HitK\n",
    "* WeightedByInteractionsHitK\n",
    "\n",
    "As K value we will use 10 (as if we recommend a box of 10 items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ready-interaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_builder.add_metric('CoverageK', 10)\n",
    "pipeline_builder.add_metric('CalibratedRecallK', 10)\n",
    "pipeline_builder.add_metric('NormalizedDiscountedCumulativeGainK', 10)\n",
    "pipeline_builder.add_metric('HitK', 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alleged-jewel",
   "metadata": {},
   "source": [
    "### Run\n",
    "\n",
    "\n",
    "To run we build the pipeline, and call `run()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pharmaceutical-socket",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = pipeline_builder.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "federal-linux",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjustable-series",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(pipeline.get_metrics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "damaged-transparency",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d8bee1e0c88f1236a37a69b0afd47a653c8dd31d18bcc42572e440933a48d476"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "metadata": {
   "interpreter": {
    "hash": "bbc584a665e03e802cc8919bac55e5568f0ebef10f66af7469634d26764da98b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
