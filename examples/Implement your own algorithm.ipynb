{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joint-import",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "\n",
    "from recpack.algorithms.base import Algorithm, TorchMLAlgorithm, FactorizationAlgorithm\n",
    "from recpack.algorithms.util import get_users, get_batches, naive_sparse2tensor, naive_tensor2sparse\n",
    "from recpack.matrix import to_binary, InteractionMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heard-negotiation",
   "metadata": {},
   "source": [
    "# Softmax over popularity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tribal-original",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from recpack.algorithms.base import Algorithm\n",
    "\n",
    "class RandomizedSoftmaxPopularity(Algorithm):\n",
    "    \"\"\"Recommend items using softmax on the natural logarithm of item counts.\n",
    "\n",
    "    Recommendations are sampled from the probability distribution\n",
    "    created by taking the softmax of the natural logarithm of item counts.\n",
    "    Items are scored such that the distance between the item in first place\n",
    "    and the item in second place is the same as between all other items.\n",
    "\n",
    "    :param K: Only the K most frequent items are considered for recommendation\n",
    "    :param tau: Temperature in the softmax computation\n",
    "    \"\"\"\n",
    "    def __init__(self, K, tau):\n",
    "        self.K = K\n",
    "        self.tau = tau\n",
    "\n",
    "    def _fit(self, X: csr_matrix):\n",
    "        # compute pop by taking logarithm of the raw counts\n",
    "        #.A1 puts it into a 1d array, making all subsequent operations easy\n",
    "        pop = np.log(np.sum(X, axis=0)).A1\n",
    "\n",
    "        max_pop = np.max(pop)\n",
    "\n",
    "        # Cut to top K\n",
    "        self.top_k_pop_items_ = np.argsort(pop)[-self.K:]\n",
    "        top_k_pop = pop[self.top_k_pop_items_]\n",
    "\n",
    "        # To make softmax numerically stable, we compute exp((pop - max(pop))/self.tau)\n",
    "        # instead of exp(pop/self.tau):\n",
    "        #\n",
    "        # softmax for item i can then be computed as\n",
    "        # e^((pop[i] - max(pop))/tau) / sum([e^(pop[j] - max(pop))/self.tau for j in topK])\n",
    "        top_k_pop_minus_max = (top_k_pop - max_pop)/self.tau\n",
    "\n",
    "        top_k_exp = np.exp(top_k_pop_minus_max)\n",
    "\n",
    "        top_k_pop_sum = np.sum(top_k_exp)\n",
    "\n",
    "        self.softmax_scores_ = top_k_exp / top_k_pop_sum    \n",
    "        \n",
    "\n",
    "    def _predict(self, X:csr_matrix):\n",
    "        # Randomly sample items, with weights decided by the softmax scores\n",
    "        users = X.nonzero()[0]\n",
    "\n",
    "        # The resulting score = (K - ix)/K\n",
    "        # The first sampled item gets score 1, and the last sampled item score 1/K\n",
    "        score_list = [\n",
    "            (u, i, (self.K-ix)/self.K)\n",
    "            for u in set(users)\n",
    "            for ix, i in enumerate(\n",
    "                np.random.choice(\n",
    "                    self.top_k_pop_items_,\n",
    "                    size=self.K,\n",
    "                    replace=False,\n",
    "                    p=self.softmax_scores_\n",
    "                )\n",
    "            )\n",
    "        ]\n",
    "        user_idxs, item_idxs, scores = list(zip(*score_list))\n",
    "        score_matrix = csr_matrix((scores, (user_idxs, item_idxs)), shape=X.shape)\n",
    "\n",
    "        return score_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opening-hostel",
   "metadata": {},
   "source": [
    "## Try it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pharmaceutical-sussex",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = [np.random.randint(0, 50) for i in range(1000)]\n",
    "items = [np.random.randint(0, 50) for i in range(1000)]\n",
    "values = [1 for i in range(1000)]\n",
    "pageviews = csr_matrix((values, (users, items)), shape=(50, 50))\n",
    "pageviews = to_binary(pageviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-cooler",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = RandomizedSoftmaxPopularity(K=20, tau=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "general-korean",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.fit(pageviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-intranet",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "algo.predict(pageviews).toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "altered-ocean",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "going-liver",
   "metadata": {},
   "source": [
    "## Recency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stunning-sharing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "\n",
    "from recpack.algorithms.base import Algorithm\n",
    "from recpack.matrix import InteractionMatrix\n",
    "\n",
    "class Recency(Algorithm):\n",
    "    def _transform_fit_input(self, X):\n",
    "        # X needs to be an InteractionMatrix for us to have access to\n",
    "        # the time of interaction at fitting time\n",
    "        self._assert_is_interaction_matrix(X)\n",
    "        self._assert_has_timestamps(X)\n",
    "        # No transformation needed\n",
    "        return X\n",
    "    \n",
    "    def _fit(self, X:InteractionMatrix):\n",
    "        # data.timestamps gives a pandas MultiIndex object, indexed by user and item,\n",
    "        # we drop the index, and group by just the item index\n",
    "        # then we select the maximal timestamp from this groupby\n",
    "        max_ts_per_item = X.timestamps.reset_index().groupby('iid')['ts'].max()\n",
    "\n",
    "        # apply min_max normalisation\n",
    "        recency = np.zeros(X.shape[1])\n",
    "        recency[max_ts_per_item.index] = max_ts_per_item.values\n",
    "\n",
    "        most_recent = np.max(recency)\n",
    "        least_recent = np.min(recency)\n",
    "\n",
    "        recency = (recency - least_recent) / (most_recent - least_recent)\n",
    "        self.recency_ = recency.copy()\n",
    "        \n",
    "    def _predict(self, X: csr_matrix):\n",
    "        results = lil_matrix(X.shape)\n",
    "\n",
    "        users = get_users(X)\n",
    "\n",
    "        results[users] = self.recency_\n",
    "\n",
    "        return results.tocsr()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overall-heater",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = 50\n",
    "num_items = 100\n",
    "num_interactions = 5000\n",
    "\n",
    "min_t = 0\n",
    "max_t = 500\n",
    "\n",
    "USER_IX = InteractionMatrix.USER_IX\n",
    "ITEM_IX = InteractionMatrix.ITEM_IX\n",
    "TIMESTAMP_IX = \"ts\"\n",
    "\n",
    "\n",
    "def data_m():\n",
    "    np.random.seed(42)\n",
    "\n",
    "    input_dict = {\n",
    "        InteractionMatrix.USER_IX: [np.random.randint(0, num_users) for _ in range(0, num_interactions)],\n",
    "        InteractionMatrix.ITEM_IX: [\n",
    "            np.random.randint(0, num_items) for _ in range(0, num_interactions)\n",
    "        ],\n",
    "        InteractionMatrix.TIMESTAMP_IX: [\n",
    "            np.random.randint(min_t, max_t) for _ in range(0, num_interactions)\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame.from_dict(input_dict)\n",
    "    df.drop_duplicates([InteractionMatrix.USER_IX, InteractionMatrix.ITEM_IX], inplace=True)\n",
    "    data = InteractionMatrix(\n",
    "        df, InteractionMatrix.ITEM_IX, InteractionMatrix.USER_IX, timestamp_ix=InteractionMatrix.TIMESTAMP_IX\n",
    "    )\n",
    "    return data\n",
    "\n",
    "data = data_m()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaging-batch",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = Recency()\n",
    "algo.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colonial-joshua",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.predict(data).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regional-fiction",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moderate-sierra",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recent-short",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-glass",
   "metadata": {},
   "source": [
    "## Torch algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perceived-membership",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from recpack.algorithms.base import TorchMLAlgorithm\n",
    "from recpack.algorithms.stopping_criterion import StoppingCriterion\n",
    "\n",
    "class MFModule(nn.Module):\n",
    "    \"\"\"MF torch module, encodes the embeddings and the forward functionality.\n",
    "\n",
    "    :param num_users: the amount of users\n",
    "    :type num_users: int\n",
    "    :param num_items: the amount of items\n",
    "    :type num_items: int\n",
    "    :param num_components: The size of the embedding per user and item, defaults to 100\n",
    "    :type num_components: int, optional\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_users, num_items, num_components=100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_components = num_components\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "\n",
    "        self.user_embedding = nn.Embedding(num_users, num_components)  # User embedding\n",
    "        self.item_embedding = nn.Embedding(num_items, num_components)  # Item embedding\n",
    "\n",
    "        self.std = 1 / num_components ** 0.5\n",
    "        # Initialise embeddings to a random start\n",
    "        nn.init.normal_(self.user_embedding.weight, std=self.std)\n",
    "        nn.init.normal_(self.item_embedding.weight, std=self.std)\n",
    "\n",
    "    def forward(\n",
    "        self, user_tensor: torch.Tensor, item_tensor: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute dot-product of user embedding (w_u) and item embedding (h_i)\n",
    "        for every user and item pair in user_tensor and item_tensor.\n",
    "\n",
    "        :param user_tensor: [description]\n",
    "        :type user_tensor: [type]\n",
    "        :param item_tensor: [description]\n",
    "        :type item_tensor: [type]\n",
    "        \"\"\"\n",
    "        w_u = self.user_embedding(user_tensor)\n",
    "        h_i = self.item_embedding(item_tensor)\n",
    "\n",
    "        return w_u.matmul(h_i.T)\n",
    "\n",
    "def my_loss(true_sim, predicted_sim):\n",
    "    \"\"\"Computes the total absolute error from predicted compared to true,\n",
    "    and averages over all users\n",
    "    \"\"\"\n",
    "    return torch.mean(torch.sum(torch.abs(true_sim - predicted_sim), axis=1))\n",
    "\n",
    "\n",
    "class SillyMF(TorchMLAlgorithm):\n",
    "    def __init__(self, batch_size, max_epochs, learning_rate, num_components=100):\n",
    "        super().__init__(\n",
    "            batch_size=batch_size,\n",
    "            max_epochs=max_epochs,\n",
    "            learning_rate=learning_rate,\n",
    "            stopping_criterion='recall',\n",
    "            seed=42\n",
    "        )\n",
    "        self.num_components = num_components\n",
    "\n",
    "    def _init_model(self, X:csr_matrix):\n",
    "        num_users, num_items = X.shape\n",
    "        self.model_ = MFModule(\n",
    "            num_users, num_items, num_components=self.num_components\n",
    "        ).to(self.device)\n",
    "\n",
    "        # We'll use a basic SGD optimiser\n",
    "        self.optimizer = optim.SGD(self.model_.parameters(), lr=self.learning_rate)\n",
    "        self.steps = 0\n",
    "\n",
    "    def _train_epoch(self, X):\n",
    "        losses = []\n",
    "        item_tensor = torch.arange(X.shape[1]).to(self.device)\n",
    "        for users in get_batches(get_users(X), batch_size=self.batch_size):\n",
    "            self.optimizer.zero_grad()\n",
    "            user_tensor = torch.LongTensor(users).to(self.device)\n",
    "            scores = self.model_.forward(user_tensor, item_tensor)\n",
    "            expected_scores = naive_sparse2tensor(X[users])\n",
    "            loss = my_loss(expected_scores, scores)\n",
    "            \n",
    "            # Backwards propagation of the loss\n",
    "            loss.backward()\n",
    "            losses.append(loss.item())\n",
    "            # Update the weight according to the gradients.\n",
    "            # All automated thanks to torch.\n",
    "            self.optimizer.step()\n",
    "            self.steps += 1\n",
    "        return losses\n",
    "            \n",
    "    def _batch_predict(self, X: csr_matrix, users: List[int] = None) -> np.ndarray:\n",
    "        \"\"\"Predict scores for matrix X, given the selected users.\n",
    "\n",
    "        If there are no selected users, you can assume X is a full matrix,\n",
    "        and users can be retrieved as the nonzero indices in the X matrix.\n",
    "\n",
    "        :param X: Matrix of user item interactions\n",
    "        :type X: csr_matrix\n",
    "        :param users: users selected for recommendation\n",
    "        :type users: List[int]\n",
    "        :return: dense matrix of scores per user item pair.\n",
    "        :rtype: np.ndarray\n",
    "        \"\"\"\n",
    "        X_pred = lil_matrix(X.shape)\n",
    "\n",
    "        if users is None:\n",
    "            users = get_users(X)\n",
    "\n",
    "        # Turn the np arrays and lists to torch tensors\n",
    "        user_tensor = torch.LongTensor(users).to(self.device)\n",
    "        item_tensor = torch.arange(X.shape[1]).to(self.device)\n",
    "\n",
    "        X_pred[users] = self.model_(user_tensor, item_tensor).detach().cpu().numpy()\n",
    "        return X_pred.tocsr()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-rolling",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recpack.scenarios import Timed\n",
    "\n",
    "algo = SillyMF(10, 3, 0.5, num_components=20)\n",
    "\n",
    "sc = Timed(400, t_validation=300, validation=True)\n",
    "sc.split(data)\n",
    "algo.fit(sc.training_data, sc.validation_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ruled-tumor",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.predict(sc.test_data[0]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apparent-revelation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "promising-browser",
   "metadata": {},
   "source": [
    "## SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "available-companion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, lil_matrix, diags\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from recpack.algorithms.base import FactorizationAlgorithm\n",
    "\n",
    "class SVD(FactorizationAlgorithm):\n",
    "    \"\"\"Singular Value Decomposition as dimension reduction recommendation algorithm.\n",
    "\n",
    "    SVD computed using the TruncatedSVD implementation from sklearn.\n",
    "    U x Sigma x V = X\n",
    "    U are the user features, and the item features are computed as Sigma x V.\n",
    "\n",
    "    :param num_components: The size of the latent dimension\n",
    "    :type num_components: int\n",
    "\n",
    "    :param random_state: The seed for the random state to allow for comparison\n",
    "    :type random_state: int\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_components=100, random_state=42):\n",
    "        super().__init__(num_components=num_components)\n",
    "\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def _fit(self, X: csr_matrix):\n",
    "        model = TruncatedSVD(\n",
    "            n_components=self.num_components, n_iter=7, random_state=self.random_state\n",
    "        )\n",
    "        # Factorization computes U x Sigma x V\n",
    "        # U are the user features,\n",
    "        # Sigma x V are the item features.\n",
    "        self.user_embedding_ = model.fit_transform(X)\n",
    "\n",
    "        V = model.components_\n",
    "        sigma = diags(model.singular_values_)\n",
    "        self.item_embedding_ = sigma @ V\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-description",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = SVD(num_components = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-walker",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southeast-biology",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183c0975",
   "metadata": {},
   "source": [
    "# Use with pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29c458e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recpack.pipelines import ALGORITHM_REGISTRY\n",
    "\n",
    "ALGORITHM_REGISTRY.register(SillyMF.__name__, SillyMF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100b8dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recpack.pipelines import PipelineBuilder\n",
    "\n",
    "pipeline_builder = PipelineBuilder()\n",
    "\n",
    "pipeline_builder.set_data_from_scenario(sc.training_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30cd32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the baseline algorithms\n",
    "# Grid parameters will be optimised using grid search before final evaluation\n",
    "pipeline_builder.add_algorithm('ItemKNN', grid={'K': [10, 20]})\n",
    "pipeline_builder.add_algorithm('EASE', grid={'l2': [10, 100, 1000], 'alpha': [0, 0.1, 0.5]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e33b121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add our new algorithm\n",
    "#Â Optimising learning rate and num_components\n",
    "# setting fixed values for max_epochs and batch_size\n",
    "pipeline_builder.add_algorithm(\n",
    "    'SillyMF',\n",
    "    grid={\n",
    "        'learning_rate': [0.1, 0.01, 0.3], \n",
    "        'num_components': [100, 200, 400]\n",
    "    },\n",
    "    params={\n",
    "        'max_epochs': 5,\n",
    "        'batch_size': 1024\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b796655",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add NDCG and Recall to be evaluated at 10, 20\n",
    "pipeline_builder.add_metric('NormalizedDiscountedCumulativeGainK', [10, 20])\n",
    "pipeline_builder.add_metric('RecallK', [10, 20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724268a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_builder.set_optimisation_metric('RecallK', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d38af0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = pipeline_builder.build()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e68161",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808f2eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.get_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6972db01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
