{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85c37828",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pytest\n",
    "import torch\n",
    "from torch import nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "471f8043",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Callable, List, Optional, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b49328c1",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import jdc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffac0039",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RecPack: An Experimental Toolkit for Recommendation Algorithms\n",
    "\n",
    "by Lien Michiels and Robin Verachtert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a80dc3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is Recpack?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6f7595",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## This Demo\n",
    "- Demonstrate implementation of new algorithm using \"Neural Matrix Factorization (He et al. 2017)\"\n",
    "- Run experiment to compare performance of new algorithm to baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4a7d08",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Matrix Factorization\n",
    "- Users and items are represented by embedding fectors\n",
    "- Similarity is modeled using an MLP network, rather than computing <u,i> as in traditional matrix factorization embedding techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad05c530",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "TODO: Include the image from the paper here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d035dea5",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "For simplicity, and to focus on the shared functionality, we implement the NMF approach that only uses an MLP, as presented in Figure 2 in the original paper.\n",
    "\n",
    "The architecture boils down to 2 embeddings, an MLP module and a final output conversion function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8824a1ff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Implementing MLP network\n",
    "Add cut picture of the MLP network\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0303cef",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In order to make the MLP reusable we will implement it as its own Torch module.\n",
    "The parameters include: layer dimensions for each layer, activation function to be used after each linear operation and the number of output nodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f0bf879",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Code used from https://github.com/facebookresearch/multimodal/blob/5dec8a/torchmultimodal/modules/layers/mlp.py\n",
    "# Another option is to use torchvision.ops.MLP \n",
    "# which is nearly identical in implementation, but is in torchvision and not in base torch.\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"A multi-layer perceptron module.\n",
    "    This module is a sequence of linear layers plus activation functions.\n",
    "    The user can optionally add normalization and/or dropout to each of the layers.\n",
    "    \n",
    "    Code used from https://github.com/facebookresearch/multimodal/blob/5dec8a/torchmultimodal/modules/layers/mlp.py\n",
    "    \n",
    "    :param in_dim: Input dimension.\n",
    "    :type in_dim: int\n",
    "    :param out_dim: Output dimension.\n",
    "    :type out_dim: int\n",
    "    :param hidden_dims: Output dimension for each hidden layer.\n",
    "    :type hidden_dims: Optional[Union[int, List[int]]] \n",
    "    :param dropout: Probability for dropout layers between each hidden layer.\n",
    "    :type dropout: float\n",
    "    :param activation: Which activation function to use. \n",
    "        Supports module type or partial.\n",
    "    :type activation: Callable[..., nn.Module]\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53ba7c0b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class MLP(MLP):\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_dim: int, \n",
    "        out_dim: int,\n",
    "        hidden_dims: Optional[Union[int, List[int]]] = None,\n",
    "        dropout: float = 0.5,\n",
    "        activation: Callable[..., nn.Module] = nn.ReLU,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = nn.ModuleList()\n",
    "\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = []\n",
    "\n",
    "        if isinstance(hidden_dims, int):\n",
    "            hidden_dims = [hidden_dims]\n",
    "\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(in_dim, hidden_dim))\n",
    "            layers.append(activation())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            in_dim = hidden_dim\n",
    "        layers.append(nn.Linear(in_dim, out_dim))\n",
    "        self.model = nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "353239bc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class MLP(MLP):\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83fcd62d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 3\n",
    "N_USERS = 5\n",
    "N_ITEMS = 10\n",
    "a = MLP(2*EMBEDDING_SIZE, N_ITEMS, [6, 4, 2], dropout=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb2a3f98",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: add tests that check that the MLP works as expected.\n",
    "def test_MLP_shapes():\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832892b7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Implementing the Neural Architecture\n",
    "\n",
    "TODO Add picture here again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55225a4",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Given the MLP module implemented before, our NeuMF module includes 2 embeddings, 1 mlp block and a final activation function to modify the scores to the 0-1 interval\n",
    "\n",
    "We also need to define the interface for the forward function.\n",
    "Since we need both a user_id and an item_id, it makes sense to have two parameters: user and items\n",
    "Both are 1D tensors of size L, that contain ids\n",
    "\n",
    "When calling forward the embeddings will be looked up for each user and item, generating two 2D tensors\n",
    "These are concatenated (horizontally) into a single matrix of dimensions L x num_components\n",
    "\n",
    "The concatenated matrix is passed through the MLP, which will predict a final score for each u,i pair.\n",
    "These scores are put into the 0, 1 interval through the use of a sigmoid function.\n",
    "\n",
    "And this is returned. Comparison to targets, and learning will happen in the baseclass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b0b73db",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class NMFModule(nn.Module):\n",
    "    \"\"\"Model that encodes the Neural Matrix Factorization Network.\n",
    "    \n",
    "    Implements the 3 tiered network defined in the He et al. paper.\n",
    "\n",
    "    :param predictive_powers: size of the last hidden layer in MLP.\n",
    "        Embedding sizes computed as 2 * predictive powers.\n",
    "    :type predictive_powers: int\n",
    "    :param n_users: number of users in the network\n",
    "    :type n_users: int\n",
    "    :param n_items: number of items in the network\n",
    "    :type n_items: int\n",
    "    :param hidden_dims: dimensions of the MLP hidden layers.\n",
    "    :type hidden_dims: Union[int, List[int]]\n",
    "    :param dropout: Dropout chance between layers of the MLP\n",
    "    :type dropout: float\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d0f5aca",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class NMFModule(NMFModule):\n",
    "    def __init__(\n",
    "        self, predictive_powers: int, n_users: int, n_items: int, dropout: float\n",
    "    ):\n",
    "        super().__init__()\n",
    "        num_components = 2 * predictive_powers\n",
    "        \n",
    "        self.user_embedding = nn.Embedding(n_users, num_components)\n",
    "        self.item_embedding = nn.Embedding(n_items, num_components)\n",
    "\n",
    "        # we use a three tiered MLP as described in the experiments of the paper.\n",
    "        hidden_dims = [\n",
    "            4 * predictive_powers, \n",
    "            2 * predictive_powers, \n",
    "            predictive_powers\n",
    "        ]\n",
    "\n",
    "        # Output is always 1, since we need a single score for u,i\n",
    "        self.mlp = MLP(4 * predictive_powers, 1, \n",
    "                       hidden_dims, dropout=dropout)\n",
    "\n",
    "        self.final = nn.Sigmoid()\n",
    "\n",
    "        # weight initialization\n",
    "        self.user_embedding.weight.data.normal_(0, \n",
    "            1.0 / self.user_embedding.embedding_dim)\n",
    "        self.item_embedding.weight.data.normal_(0, \n",
    "            1.0 / self.item_embedding.embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "551d5c5b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class NMFModule(NMFModule):\n",
    "    def forward(self, users: torch.LongTensor, items: torch.LongTensor) -> torch.FloatTensor:\n",
    "        \"\"\"Predict scores for the user item pairs obtained \n",
    "        by zipping together the two inputs\n",
    "\n",
    "        :param users: 1D tensor with user ids\n",
    "        :type users: torch.LongTensor\n",
    "        :param items: 1D tensor with item ids\n",
    "        :type items: torch.LongTensor\n",
    "        :return: 1D tensor with predicted similarities.\n",
    "            Position i is the similarity between \n",
    "            `users[i]` and `items[i]`\n",
    "        :rtype: torch.FloatTensor\n",
    "        \"\"\"\n",
    "\n",
    "        # Embedding lookups\n",
    "        user_emb = self.user_embedding(users)\n",
    "        item_emb = self.item_embedding(items)\n",
    "\n",
    "        # Pass concatenated through MLP and apply sigmoid\n",
    "        return self.final(\n",
    "            self.mlp(\n",
    "                torch.hstack([user_emb, item_emb])\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8bcb491",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def test_output_shapes_NMF(\n",
    "    embedding_size, num_users, num_items, hidden_sizes\n",
    "):\n",
    "    \"\"\"Check that no mather the inner settings of the network, the output is always correct\"\"\"\n",
    "    mod = NMFModule(embedding_size, num_users, num_items, hidden_sizes, 0.0)\n",
    "    \n",
    "    user_tensor = torch.LongTensor([1, 2])\n",
    "    item_tensor = torch.LongTensor([1, 2])\n",
    "    \n",
    "    res = mod(user_tensor, item_tensor) # predict scores for items given the users\n",
    "    \n",
    "    assert res.shape == (2, 1)\n",
    "\n",
    "    assert (res.detach().numpy() <= 1).all()\n",
    "    assert (res.detach().numpy() >= 0).all()\n",
    "\n",
    "\n",
    "test_output_shapes_NMF(5, 10, 10, [10])\n",
    "test_output_shapes_NMF(5, 10, 10, [10, 5, 3])\n",
    "test_output_shapes_NMF(5, 3, 10, [10])\n",
    "test_output_shapes_NMF(1, 3, 3, [10])\n",
    "\n",
    "\n",
    "# def test_shape_input_check_NMF():\n",
    "#     \"\"\"Check that no mather the inner settings of the network, the output is always correct\"\"\"\n",
    "    \n",
    "#     mod = NMFModule(3, 3, 3, [6], 0.0)\n",
    "    \n",
    "#     user_tensor = torch.LongTensor([[1, 2]])\n",
    "#     item_tensor = torch.LongTensor([[1, 2]])\n",
    "    \n",
    "#     with pytest.raises(ValueError):\n",
    "#         mod(user_tensor, item_tensor)\n",
    "        \n",
    "#     user_tensor = torch.LongTensor([1, 2])\n",
    "#     item_tensor = torch.LongTensor([1, 2, 0])\n",
    "    \n",
    "#     with pytest.raises(ValueError):\n",
    "#         mod(user_tensor, item_tensor)\n",
    "\n",
    "#     user_tensor = torch.LongTensor([[1, 2]])\n",
    "#     item_tensor = torch.LongTensor([1, 2])\n",
    "\n",
    "#     with pytest.raises(ValueError):\n",
    "#         mod(user_tensor, item_tensor)\n",
    "\n",
    "\n",
    "# test_shape_input_check_NMF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c07d570",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Union, Optional\n",
    "\n",
    "import pandas as pd\n",
    "from recpack.algorithms.base import TorchMLAlgorithm\n",
    "from recpack.algorithms.samplers import PositiveNegativeSampler\n",
    "from recpack.algorithms.util import get_users\n",
    "from recpack.data.matrix import InteractionMatrix\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5e136a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Implementing the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f734851a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Choosing the right baseclass\n",
    "`TorchMLAlgorithm`\n",
    "- Implements common functionality\n",
    "    - Fitting:\n",
    "        - Epoch loop\n",
    "        - Early stopping\n",
    "        - Keep best/last model\n",
    "        - Progress logs\n",
    "    - Prediction\n",
    "        - Batched prediction loop\n",
    "        - Prune recommendations \n",
    "    - Saving + loading\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86035a9d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Choosing the right baseclass\n",
    "We need to implement:\n",
    "\n",
    "- `__init__`: Sets the hyperparameters\n",
    "- `_init_model`: Initialises the model during training\n",
    "- `_train_epoch`: train for a single epoch\n",
    "- `predict_batch`: predict for a batch of users\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff101b99",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class NeuMF(TorchMLAlgorithm):\n",
    "    \"\"\"Implementation of Neural Matrix Factoration.\n",
    "\n",
    "    Neural Matrix Factorization based on MLP architecture\n",
    "    as presented in Figure 2 in He, Xiangnan, et al. \n",
    "    \"Neural collaborative filtering.\"\n",
    "    In Proceedings of the 26th international conference on world wide web. 2017.\n",
    "\n",
    "    Represents the users and items using an embedding, \n",
    "    similarity between the two is modelled using a neural network.\n",
    "\n",
    "    The network consists of an embedding for both users and items.\n",
    "    To compute similarity those two embeddings are \n",
    "    concatenated and passed through the MLP\n",
    "    Finally the similarity is transformed to the [0,1] domain\n",
    "    using a sigmoid function.\n",
    "\n",
    "    As in the paper, the sum of square errors is used as loss function.\n",
    "    Positive items should get a prediction close to 1, \n",
    "    while sampled negatives should get a value close to 0.\n",
    "\n",
    "    The MLP has 3 layers, as suggested in the experiments section.\n",
    "    Bottom layer has dimension `4 * predictive_powers`, \n",
    "    middle layer `2 * predictive_powers`\n",
    "    and the top layer has `predictive_powers`.\n",
    "\n",
    "    :param predictive_powers: Size of the last hidden layer in the MLP network.\n",
    "        Embedding size is 2 * predictive_powers\n",
    "    :type predictive_powers: int\n",
    "    :param batch_size: How many samples to use in each update step.\n",
    "        Higher batch sizes make each epoch more efficient,\n",
    "        but increases the amount of epochs needed to converge to the optimum,\n",
    "        by reducing the amount of updates per epoch.\n",
    "        Defaults to 512.\n",
    "    :type batch_size: Optional[int]\n",
    "    :param max_epochs: The max number of epochs to train.\n",
    "        If the stopping criterion uses early stopping, less epochs could be used.\n",
    "        Defaults to 10.\n",
    "    :type max_epochs: Optional[int]\n",
    "    :param learning_rate: How much to update the weights at each update. Defaults to 0.01\n",
    "    :type learning_rate: Optional[float]\n",
    "    :param stopping_criterion: Name of the stopping criterion to use for training.\n",
    "        For available values,\n",
    "        check :meth:`recpack.algorithms.stopping_criterion.StoppingCriterion.FUNCTIONS`\n",
    "        Defaults to 'ndcg'\n",
    "    :type stopping_criterion: Optional[str]\n",
    "    :param stop_early: If True, early stopping is enabled,\n",
    "        and after ``max_iter_no_change`` iterations where improvement of loss function\n",
    "        is below ``min_improvement`` the optimisation is stopped,\n",
    "        even if max_epochs is not reached.\n",
    "        Defaults to False\n",
    "    :type stop_early: bool, optional\n",
    "    :param max_iter_no_change: If early stopping is enabled,\n",
    "        stop after this amount of iterations without change.\n",
    "        Defaults to 5\n",
    "    :type max_iter_no_change: int, optional\n",
    "    :param min_improvement: If early stopping is enabled, no change is detected,\n",
    "        if the improvement is below this value.\n",
    "        Defaults to 0.01\n",
    "    :type min_improvement: float, optional\n",
    "    :param seed: Seed to the randomizers, useful for reproducible results,\n",
    "        defaults to None\n",
    "    :type seed: int, optional\n",
    "    :param save_best_to_file: If true, the best model will be saved after training,\n",
    "        defaults to False\n",
    "    :type save_best_to_file: bool, optional\n",
    "    :param keep_last: Retain last model, rather than best\n",
    "        (according to stopping criterion value on validation data), defaults to False\n",
    "    :type keep_last: bool, optional\n",
    "    :param predict_topK: The topK recommendations to keep per row in the matrix.\n",
    "        Use when the user x item output matrix would become too large for RAM.\n",
    "        Defaults to None, which results in no filtering.\n",
    "    :type predict_topK: int, optional\n",
    "    :param U: Amount of negatives to sample for each positive example, defaults to 1\n",
    "    :type U: int, optional\n",
    "    :param dropout: Dropout parameter used in MLP, defaults to 0.0\n",
    "    :type dropout: float, optional\n",
    "    :param exact_sampling: Enable or disable exact checks while sampling. \n",
    "        With exact sampling the sampled negatives are guaranteed to not have been visited by the user. \n",
    "        Non exact sampling assumes that the space for item selection is large enough, \n",
    "        such that most items are likely not seen before.\n",
    "        Defaults to False,\n",
    "    :type exact_sampling: bool, optional\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0d4642d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class NeuMF(NeuMF):\n",
    "    def __init__(\n",
    "        self,\n",
    "        predictive_powers: int,\n",
    "        batch_size: Optional[int] = 512,\n",
    "        max_epochs: Optional[int] = 10,\n",
    "        learning_rate: Optional[float] = 0.01,\n",
    "        stopping_criterion: Optional[str] = \"ndcg\",\n",
    "        stop_early: Optional[bool] = False,\n",
    "        max_iter_no_change: Optional[int] = 5,\n",
    "        min_improvement: Optional[float] = 0.0,\n",
    "        seed: Optional[int] = None,\n",
    "        save_best_to_file: Optional[bool] = False,\n",
    "        keep_last: Optional[bool] = False,\n",
    "        predict_topK: Optional[int] = None,\n",
    "        U: Optional[int] = 1,\n",
    "        dropout: Optional[float] = 0.0,\n",
    "        exact_sampling: Optional[bool] = False,\n",
    "    ):\n",
    "        super().__init__(batch_size, max_epochs, learning_rate,\n",
    "            stopping_criterion, stop_early, max_iter_no_change,\n",
    "            min_improvement, seed, save_best_to_file, keep_last,\n",
    "            predict_topK,\n",
    "        )\n",
    "\n",
    "        self.predictive_powers = predictive_powers\n",
    "\n",
    "        self.U = U\n",
    "        self.dropout = dropout\n",
    "        self.exact_sampling = exact_sampling\n",
    "\n",
    "        self.sampler = PositiveNegativeSampler(\n",
    "            U=self.U, replace=False, exact=exact_sampling, \n",
    "            batch_size=self.batch_size\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8b3945",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We choose to stick to the default 3 hidden layer MLP they use in their experiments, and thus reduce complexity of the code.\n",
    "\n",
    "We also decided to not have configurable parameters for the hidden sizes and predictive factors, instead we only give 1 parameter for defining the structure of the MLP, namely the embedding size num_components. Multiplying it by 2 gives the bottom layer size of the MLP, while dividing it by 2 gives the predictive factors.\n",
    "\n",
    "The second non standard parameter is U, the number of negatives to sample when training the model. For each positive, U negatives will be sampled.\n",
    "\n",
    "We choose to take the embedding size as the parameter that defines the \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8598b7b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class NeuMF(NeuMF):\n",
    "    def _init_model(self, X: csr_matrix):\n",
    "        num_users, num_items = X.shape\n",
    "        self.model_ = NMFModule(self.num_components, num_users, num_items, self.dropout).to(\n",
    "            self.device\n",
    "        )\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.model_.parameters(), lr=self.learning_rate\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec501f45",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class NeuMF(NeuMF):\n",
    "    def _train_epoch(self, X: csr_matrix) -> List[int]:\n",
    "        losses = []\n",
    "        for users, positives, negatives in self.sampler.sample(X):\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Predict for the positives\n",
    "            positive_scores = self.model_.forward(\n",
    "                users.to(self.device), positives.to(self.device))\n",
    "            # Predict for the negatives\n",
    "            negative_scores = self.model_.forward(\n",
    "                *self._construct_negative_prediction_input(\n",
    "                    users.to(self.device), negatives.to(self.device))\n",
    "            )\n",
    "\n",
    "            loss = self._compute_loss(\n",
    "                positive_scores, negative_scores)\n",
    "\n",
    "            # Backwards propagation of the loss\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        return losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cfb90f",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "During a single epoch the algorithm will loop in batch sizes through the positives in random order. For each positive user, item pair also U negatives will be sampled. \n",
    "\n",
    "This is all handled by a RecPack preimplemented Sampler: PositiveNegativeSampler\n",
    "\n",
    "For each batch we will\n",
    "compute similarities between the user u and the sampled positives and negatives given the 1D tensors for users and positives, and the 2D tensors for negatives.\n",
    "\n",
    "For the positives this is straightforward since the tensors are already in the right format.\n",
    "For the negatives we need to restructure the 2D tensor into a 1D tensor\n",
    "\n",
    "Once we have computed similarities for both positives and negatives, we can compute the loss, which we will implement next.\n",
    "\n",
    "Given the loss we let torch handle the update to the network by back propagation.\n",
    "\n",
    "We finally store the loss, so we can log the average loss to the user, allowing them to monitor the training.\n",
    "\n",
    "The array of losses accumulated during the epoch is returned as return value of the train epoch function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f59b81c0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class NeuMF(NeuMF):\n",
    "    def _compute_loss(\n",
    "        self, positive_scores: torch.FloatTensor, negative_scores: torch.FloatTensor\n",
    "    ) -> torch.FloatTensor:\n",
    "        \"\"\"Compute the Square Error loss given recommendations \n",
    "        for positive items, and sampled negatives.\n",
    "        \"\"\"\n",
    "\n",
    "        mse = nn.MSELoss(reduction=\"sum\")\n",
    "        return mse(positive_scores, torch.ones_like(positive_scores, dtype=torch.float)) + mse(\n",
    "            negative_scores, torch.zeros_like(negative_scores, dtype=torch.float)\n",
    "        )\n",
    "\n",
    "    def _construct_negative_prediction_input(self, users, negatives):\n",
    "        \"\"\"Construct the prediction input given a 1D user tensor and a 2D negatives tensor.\n",
    "        \n",
    "        Since negatives has shape |batch| x U, and users is a 1d vector,\n",
    "        these need to be turned into two 1D vectors of shape |batch| * U\n",
    "\n",
    "        First the users as a row are stacked U times and transposed,\n",
    "        so that this is also a batch x U tensor.\n",
    "        Then both are reshaped to remove the 2nd dimension, \n",
    "        resulting in a single long 1d vector.\n",
    "        \"\"\"\n",
    "        return users.repeat(self.U, 1).T.reshape(-1), negatives.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a451a82a",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Squared Loss. This is simply the sum of the square of the error in the prediction. Because we are reconstructing a binary matrix, the target value for positives is 1 and 0 for the negatives.\n",
    "\n",
    "construction of input:\n",
    "repeat the users tensor such that the correct users are still associated with the correct negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b93dbaa3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class NeuMF(NeuMF):\n",
    "    def _batch_predict(self, X: csr_matrix, users: List[int]) -> csr_matrix:\n",
    "        \"\"\"Generate recommendations for each of the users.\"\"\"\n",
    "\n",
    "        X_pred = lil_matrix(X.shape)\n",
    "        if users is None:\n",
    "            users = get_users(X)\n",
    "\n",
    "        _, n_items = X.shape\n",
    "        n_users = len(users)\n",
    "\n",
    "        # Create two 1D arrays such that each item gets a score for each of the users.\n",
    "        # The user tensor contains the users in order (eg. [1, 1, 2, 2]), \n",
    "        # such that the items are the item indices repeated (eg. [0, 1, 2, 0, 1, 2]).\n",
    "        user_tensor = torch.LongTensor(users).repeat(n_items, 1).T.reshape(-1).to(self.device)\n",
    "        item_tensor = torch.arange(n_items).repeat(n_users).to(self.device)\n",
    "\n",
    "        X_pred[users] = self.model_(user_tensor, item_tensor).detach().cpu().numpy().reshape(n_users, n_items)\n",
    "        return X_pred.tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b087b9",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "For any decent size dataset computing a prediction for each user at once requires too much RAM to be feasible. \n",
    "Instead we compute the recommendations for a batch of users, and then only keep topK recommendations.\n",
    "\n",
    "In the implementation our only task is to generate the correct tensors to give as input to the module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ca8f574",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "TIMESTAMP_IX = 'ts'\n",
    "ITEM_IX = 'iid'\n",
    "USER_IX = 'uid'\n",
    "\n",
    "data = {\n",
    "    TIMESTAMP_IX: [3, 2, 1, 4, 0, 1, 2, 4, 0, 1, 2],\n",
    "    ITEM_IX: [0, 1, 2, 3, 0, 1, 2, 4, 0, 1, 2],\n",
    "    USER_IX: [0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5],\n",
    "}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "mat = InteractionMatrix(df, ITEM_IX, USER_IX, timestamp_ix=TIMESTAMP_IX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1cf3329d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     params \u001b[38;5;241m=\u001b[39m [np \u001b[38;5;28;01mfor\u001b[39;00m np \u001b[38;5;129;01min\u001b[39;00m a\u001b[38;5;241m.\u001b[39mmodel_\u001b[38;5;241m.\u001b[39mnamed_parameters() \u001b[38;5;28;01mif\u001b[39;00m np[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mrequires_grad]\n\u001b[1;32m     18\u001b[0m     assert_changed(params_before, params, device)\n\u001b[0;32m---> 20\u001b[0m \u001b[43mtest_training_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_values\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36mtest_training_epoch\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     16\u001b[0m     a\u001b[38;5;241m.\u001b[39m_train_epoch(X)\n\u001b[1;32m     17\u001b[0m params \u001b[38;5;241m=\u001b[39m [np \u001b[38;5;28;01mfor\u001b[39;00m np \u001b[38;5;129;01min\u001b[39;00m a\u001b[38;5;241m.\u001b[39mmodel_\u001b[38;5;241m.\u001b[39mnamed_parameters() \u001b[38;5;28;01mif\u001b[39;00m np[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mrequires_grad]\n\u001b[0;32m---> 18\u001b[0m \u001b[43massert_changed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_before\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dist-env/lib/python3.8/site-packages/recpack/tests/test_algorithms/util.py:8\u001b[0m, in \u001b[0;36massert_changed\u001b[0;34m(params_before, params_after, device)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21massert_changed\u001b[39m(params_before, params_after, device):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# check if variables have changed\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (_, p0), (_, p1) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(params_before, params_after):\n\u001b[0;32m----> 8\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mequal(p0\u001b[38;5;241m.\u001b[39mto(device), p1\u001b[38;5;241m.\u001b[39mto(device))\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from recpack.tests.test_algorithms.util import assert_changed, assert_same\n",
    "\n",
    "def test_training_epoch(X):\n",
    "    a = NeuMF(\n",
    "        num_components=4, \n",
    "        U=2,\n",
    "        exact_sampling=True\n",
    "    )\n",
    "    device = a.device\n",
    "    a._init_model(X)\n",
    "\n",
    "    # Each training epoch should update the parameters\n",
    "    params = [np for np in a.model_.named_parameters() if np[1].requires_grad]\n",
    "    params_before = [(name, p.clone()) for (name, p) in params]\n",
    "    for _ in range(5):\n",
    "        a._train_epoch(X)\n",
    "    params = [np for np in a.model_.named_parameters() if np[1].requires_grad]\n",
    "    assert_changed(params_before, params, device)\n",
    "\n",
    "test_training_epoch(mat.binary_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "814b0473",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-15 05:51:37,505 - base - recpack - INFO - Processed epoch 0 in 0.01 s.Batch Training Loss = 10.4516\n",
      "2022-06-15 05:51:37,517 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.6405398473870061, which is better than previous iterations.\n",
      "2022-06-15 05:51:37,517 - base - recpack - INFO - Model improved. Storing better model.\n",
      "2022-06-15 05:51:37,523 - base - recpack - INFO - Evaluation at end of 0 took 0.02 s.\n",
      "2022-06-15 05:51:37,533 - base - recpack - INFO - Processed epoch 1 in 0.01 s.Batch Training Loss = 10.3924\n",
      "2022-06-15 05:51:37,538 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.6028241164651866, which is worse than previous iterations.\n",
      "2022-06-15 05:51:37,538 - base - recpack - INFO - Evaluation at end of 1 took 0.00 s.\n",
      "2022-06-15 05:51:37,550 - base - recpack - INFO - Processed epoch 2 in 0.01 s.Batch Training Loss = 10.3358\n",
      "2022-06-15 05:51:37,555 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.6028241164651866, which is worse than previous iterations.\n",
      "2022-06-15 05:51:37,556 - base - recpack - INFO - Evaluation at end of 2 took 0.00 s.\n",
      "2022-06-15 05:51:37,571 - base - recpack - INFO - Processed epoch 3 in 0.01 s.Batch Training Loss = 10.2785\n",
      "2022-06-15 05:51:37,576 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.6028241164651866, which is worse than previous iterations.\n",
      "2022-06-15 05:51:37,576 - base - recpack - INFO - Evaluation at end of 3 took 0.00 s.\n",
      "2022-06-15 05:51:37,592 - base - recpack - INFO - Processed epoch 4 in 0.02 s.Batch Training Loss = 10.2206\n",
      "2022-06-15 05:51:37,597 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.6028241164651866, which is worse than previous iterations.\n",
      "2022-06-15 05:51:37,598 - base - recpack - INFO - Evaluation at end of 4 took 0.00 s.\n",
      "2022-06-15 05:51:37,611 - base - recpack - INFO - Processed epoch 5 in 0.01 s.Batch Training Loss = 10.1621\n",
      "2022-06-15 05:51:37,615 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.6028241164651866, which is worse than previous iterations.\n",
      "2022-06-15 05:51:37,616 - base - recpack - INFO - Evaluation at end of 5 took 0.00 s.\n",
      "2022-06-15 05:51:37,629 - base - recpack - INFO - Processed epoch 6 in 0.01 s.Batch Training Loss = 10.1032\n",
      "2022-06-15 05:51:37,634 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.6028241164651866, which is worse than previous iterations.\n",
      "2022-06-15 05:51:37,634 - base - recpack - INFO - Evaluation at end of 6 took 0.00 s.\n",
      "2022-06-15 05:51:37,650 - base - recpack - INFO - Processed epoch 7 in 0.01 s.Batch Training Loss = 10.0437\n",
      "2022-06-15 05:51:37,654 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.6028241164651866, which is worse than previous iterations.\n",
      "2022-06-15 05:51:37,655 - base - recpack - INFO - Evaluation at end of 7 took 0.00 s.\n",
      "2022-06-15 05:51:37,665 - base - recpack - INFO - Processed epoch 8 in 0.01 s.Batch Training Loss = 9.9837\n",
      "2022-06-15 05:51:37,670 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.6028241164651866, which is worse than previous iterations.\n",
      "2022-06-15 05:51:37,671 - base - recpack - INFO - Evaluation at end of 8 took 0.00 s.\n",
      "2022-06-15 05:51:37,681 - base - recpack - INFO - Processed epoch 9 in 0.01 s.Batch Training Loss = 9.9233\n",
      "2022-06-15 05:51:37,686 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.6028241164651866, which is worse than previous iterations.\n",
      "2022-06-15 05:51:37,687 - base - recpack - INFO - Evaluation at end of 9 took 0.00 s.\n",
      "2022-06-15 05:51:37,690 - base - recpack - INFO - Fitting NeuMF complete - Took 0.205s\n",
      "2022-06-15 05:51:37,709 - base - recpack - INFO - Processed epoch 0 in 0.01 s.Batch Training Loss = 9.9468\n",
      "2022-06-15 05:51:37,714 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.7737582122087544, which is better than previous iterations.\n",
      "2022-06-15 05:51:37,715 - base - recpack - INFO - Model improved. Storing better model.\n",
      "2022-06-15 05:51:37,718 - base - recpack - INFO - Evaluation at end of 0 took 0.01 s.\n",
      "2022-06-15 05:51:37,734 - base - recpack - INFO - Processed epoch 1 in 0.02 s.Batch Training Loss = 9.8602\n",
      "2022-06-15 05:51:37,739 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.7372530491956414, which is worse than previous iterations.\n",
      "2022-06-15 05:51:37,740 - base - recpack - INFO - Evaluation at end of 1 took 0.00 s.\n",
      "2022-06-15 05:51:37,754 - base - recpack - INFO - Processed epoch 2 in 0.01 s.Batch Training Loss = 9.8002\n",
      "2022-06-15 05:51:37,759 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.7301688035606183, which is worse than previous iterations.\n",
      "2022-06-15 05:51:37,760 - base - recpack - INFO - Evaluation at end of 2 took 0.00 s.\n",
      "2022-06-15 05:51:37,771 - base - recpack - INFO - Processed epoch 3 in 0.01 s.Batch Training Loss = 9.7273\n",
      "2022-06-15 05:51:37,776 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.7301688035606183, which is worse than previous iterations.\n",
      "2022-06-15 05:51:37,776 - base - recpack - INFO - Evaluation at end of 3 took 0.00 s.\n",
      "2022-06-15 05:51:37,790 - base - recpack - INFO - Processed epoch 4 in 0.01 s.Batch Training Loss = 9.6632\n",
      "2022-06-15 05:51:37,795 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.7443372948306645, which is worse than previous iterations.\n",
      "2022-06-15 05:51:37,796 - base - recpack - INFO - Evaluation at end of 4 took 0.00 s.\n",
      "2022-06-15 05:51:37,807 - base - recpack - INFO - Processed epoch 5 in 0.01 s.Batch Training Loss = 9.5924\n",
      "2022-06-15 05:51:37,812 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.7372530491956414, which is worse than previous iterations.\n",
      "2022-06-15 05:51:37,813 - base - recpack - INFO - Evaluation at end of 5 took 0.00 s.\n",
      "2022-06-15 05:51:37,825 - base - recpack - INFO - Processed epoch 6 in 0.01 s.Batch Training Loss = 9.5142\n",
      "2022-06-15 05:51:37,829 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.7372530491956414, which is worse than previous iterations.\n",
      "2022-06-15 05:51:37,830 - base - recpack - INFO - Evaluation at end of 6 took 0.00 s.\n",
      "2022-06-15 05:51:37,842 - base - recpack - INFO - Processed epoch 7 in 0.01 s.Batch Training Loss = 9.4398\n",
      "2022-06-15 05:51:37,847 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.7372530491956414, which is worse than previous iterations.\n",
      "2022-06-15 05:51:37,847 - base - recpack - INFO - Evaluation at end of 7 took 0.00 s.\n",
      "2022-06-15 05:51:37,867 - base - recpack - INFO - Processed epoch 8 in 0.02 s.Batch Training Loss = 9.3403\n",
      "2022-06-15 05:51:37,872 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.7372530491956414, which is worse than previous iterations.\n",
      "2022-06-15 05:51:37,873 - base - recpack - INFO - Evaluation at end of 8 took 0.01 s.\n",
      "2022-06-15 05:51:37,885 - base - recpack - INFO - Processed epoch 9 in 0.01 s.Batch Training Loss = 9.2433\n",
      "2022-06-15 05:51:37,890 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.7372530491956414, which is worse than previous iterations.\n",
      "2022-06-15 05:51:37,890 - base - recpack - INFO - Evaluation at end of 9 took 0.01 s.\n",
      "2022-06-15 05:51:37,894 - base - recpack - INFO - Fitting NeuMF complete - Took 0.198s\n",
      "2022-06-15 05:51:37,910 - base - recpack - INFO - Processed epoch 0 in 0.01 s.Batch Training Loss = 9.6960\n",
      "2022-06-15 05:51:37,915 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.7837146108282446, which is better than previous iterations.\n",
      "2022-06-15 05:51:37,916 - base - recpack - INFO - Model improved. Storing better model.\n",
      "2022-06-15 05:51:37,919 - base - recpack - INFO - Evaluation at end of 0 took 0.01 s.\n",
      "2022-06-15 05:51:37,930 - base - recpack - INFO - Processed epoch 1 in 0.01 s.Batch Training Loss = 9.5650\n",
      "2022-06-15 05:51:37,935 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.7863204548293853, which is better than previous iterations.\n",
      "2022-06-15 05:51:37,936 - base - recpack - INFO - Model improved. Storing better model.\n",
      "2022-06-15 05:51:37,938 - base - recpack - INFO - Evaluation at end of 1 took 0.01 s.\n",
      "2022-06-15 05:51:37,958 - base - recpack - INFO - Processed epoch 2 in 0.02 s.Batch Training Loss = 9.4404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-15 05:51:37,963 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.7838062179781108, which is worse than previous iterations.\n",
      "2022-06-15 05:51:37,964 - base - recpack - INFO - Evaluation at end of 2 took 0.00 s.\n",
      "2022-06-15 05:51:37,973 - base - recpack - INFO - Processed epoch 3 in 0.01 s.Batch Training Loss = 9.3179\n",
      "2022-06-15 05:51:37,978 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.7659479478689263, which is worse than previous iterations.\n",
      "2022-06-15 05:51:37,979 - base - recpack - INFO - Evaluation at end of 3 took 0.01 s.\n",
      "2022-06-15 05:51:37,990 - base - recpack - INFO - Processed epoch 4 in 0.01 s.Batch Training Loss = 9.2018\n",
      "2022-06-15 05:51:37,995 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.8524021713512242, which is better than previous iterations.\n",
      "2022-06-15 05:51:37,996 - base - recpack - INFO - Model improved. Storing better model.\n",
      "2022-06-15 05:51:37,999 - base - recpack - INFO - Evaluation at end of 4 took 0.01 s.\n",
      "2022-06-15 05:51:38,018 - base - recpack - INFO - Processed epoch 5 in 0.02 s.Batch Training Loss = 9.0883\n",
      "2022-06-15 05:51:38,023 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.8319380572408991, which is worse than previous iterations.\n",
      "2022-06-15 05:51:38,023 - base - recpack - INFO - Evaluation at end of 5 took 0.00 s.\n",
      "2022-06-15 05:51:38,044 - base - recpack - INFO - Processed epoch 6 in 0.02 s.Batch Training Loss = 8.9781\n",
      "2022-06-15 05:51:38,048 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.6988666359953621, which is worse than previous iterations.\n",
      "2022-06-15 05:51:38,049 - base - recpack - INFO - Evaluation at end of 6 took 0.00 s.\n",
      "2022-06-15 05:51:38,061 - base - recpack - INFO - Processed epoch 7 in 0.01 s.Batch Training Loss = 8.8724\n",
      "2022-06-15 05:51:38,066 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.6610039614973312, which is worse than previous iterations.\n",
      "2022-06-15 05:51:38,067 - base - recpack - INFO - Evaluation at end of 7 took 0.00 s.\n",
      "2022-06-15 05:51:38,077 - base - recpack - INFO - Processed epoch 8 in 0.01 s.Batch Training Loss = 8.7675\n",
      "2022-06-15 05:51:38,082 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.6232882305755117, which is worse than previous iterations.\n",
      "2022-06-15 05:51:38,083 - base - recpack - INFO - Evaluation at end of 8 took 0.00 s.\n",
      "2022-06-15 05:51:38,097 - base - recpack - INFO - Processed epoch 9 in 0.01 s.Batch Training Loss = 8.6667\n",
      "2022-06-15 05:51:38,101 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.6246457420604296, which is worse than previous iterations.\n",
      "2022-06-15 05:51:38,102 - base - recpack - INFO - Evaluation at end of 9 took 0.00 s.\n",
      "2022-06-15 05:51:38,105 - base - recpack - INFO - Fitting NeuMF complete - Took 0.208s\n"
     ]
    }
   ],
   "source": [
    "def test_batch_predict(mat, users):\n",
    "    a = NeuMF(\n",
    "        num_components=4, \n",
    "        U=2,\n",
    "        exact_sampling=True\n",
    "    )\n",
    "    device = a.device\n",
    "    a.fit(mat, (mat, mat))\n",
    "    params = [np for np in a.model_.named_parameters() if np[1].requires_grad]\n",
    "    params_before = [(name, p.clone()) for (name, p) in params]\n",
    "\n",
    "    pred = a._batch_predict(mat.users_in(users), users=users)\n",
    "\n",
    "    assert pred.shape == mat.shape\n",
    "    np.testing.assert_array_equal(pred.sum(axis=1).nonzero()[0], users)\n",
    "\n",
    "    params = [np for np in a.model_.named_parameters() if np[1].requires_grad]\n",
    "    assert_same(params_before, params, device)\n",
    "\n",
    "    \n",
    "\n",
    "test_batch_predict(mat, [0, 1])\n",
    "test_batch_predict(mat, [0])\n",
    "test_batch_predict(mat, [0, 1, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed4d2e7d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def test_negative_input_construction(users, negatives, U):\n",
    "    \n",
    "    a = NeuMF(\n",
    "        num_components=8, \n",
    "        U=U\n",
    "    )\n",
    "    \n",
    "    num_users = users.shape[0]\n",
    "    users_input, negatives_input = a._construct_negative_prediction_input(users, negatives)\n",
    "    assert users_input.shape == negatives_input.shape\n",
    "    assert len(users_input.shape) == 1 # 1d vectors\n",
    "    \n",
    "    # Check that both are in the right order (each user is repeated U times before the next user is present)\n",
    "    for ix in range(users_input.shape[0]):\n",
    "        assert users_input[ix] == users[ix // U]\n",
    "        assert negatives_input[ix] == negatives[ix // U, ix % U]\n",
    "\n",
    "test_negative_input_construction(torch.LongTensor([4, 5, 6]), torch.LongTensor([[1, 2], [1, 2], [1, 2]]), U=2)\n",
    "test_negative_input_construction(torch.LongTensor([4, 5, 6]), torch.LongTensor([[1], [1], [1]]), U=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f31c898",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-15 05:51:41,253 - base - recpack - INFO - Processed epoch 0 in 0.04 s.Batch Training Loss = 0.5015\n",
      "2022-06-15 05:51:41,265 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.7633421038677857, which is better than previous iterations.\n",
      "2022-06-15 05:51:41,266 - base - recpack - INFO - Model improved. Storing better model.\n",
      "2022-06-15 05:51:41,269 - base - recpack - INFO - Evaluation at end of 0 took 0.02 s.\n",
      "2022-06-15 05:51:41,317 - base - recpack - INFO - Processed epoch 1 in 0.05 s.Batch Training Loss = 0.5001\n",
      "2022-06-15 05:51:41,328 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.7819889967717142, which is better than previous iterations.\n",
      "2022-06-15 05:51:41,329 - base - recpack - INFO - Model improved. Storing better model.\n",
      "2022-06-15 05:51:41,332 - base - recpack - INFO - Evaluation at end of 1 took 0.01 s.\n",
      "2022-06-15 05:51:41,380 - base - recpack - INFO - Processed epoch 2 in 0.05 s.Batch Training Loss = 0.4998\n",
      "2022-06-15 05:51:41,394 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.7908904636131339, which is better than previous iterations.\n",
      "2022-06-15 05:51:41,394 - base - recpack - INFO - Model improved. Storing better model.\n",
      "2022-06-15 05:51:41,398 - base - recpack - INFO - Evaluation at end of 2 took 0.02 s.\n",
      "2022-06-15 05:51:41,446 - base - recpack - INFO - Processed epoch 3 in 0.05 s.Batch Training Loss = 0.4977\n",
      "2022-06-15 05:51:41,458 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.7690688380178908, which is worse than previous iterations.\n",
      "2022-06-15 05:51:41,459 - base - recpack - INFO - Evaluation at end of 3 took 0.01 s.\n",
      "2022-06-15 05:51:41,504 - base - recpack - INFO - Processed epoch 4 in 0.04 s.Batch Training Loss = 0.4829\n",
      "2022-06-15 05:51:41,516 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.7690688380178909, which is worse than previous iterations.\n",
      "2022-06-15 05:51:41,516 - base - recpack - INFO - Evaluation at end of 4 took 0.01 s.\n",
      "2022-06-15 05:51:41,565 - base - recpack - INFO - Processed epoch 5 in 0.05 s.Batch Training Loss = 0.4504\n",
      "2022-06-15 05:51:41,577 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.7690688380178909, which is worse than previous iterations.\n",
      "2022-06-15 05:51:41,578 - base - recpack - INFO - Evaluation at end of 5 took 0.01 s.\n",
      "2022-06-15 05:51:41,627 - base - recpack - INFO - Processed epoch 6 in 0.05 s.Batch Training Loss = 0.4387\n",
      "2022-06-15 05:51:41,639 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.743126726921958, which is worse than previous iterations.\n",
      "2022-06-15 05:51:41,639 - base - recpack - INFO - Evaluation at end of 6 took 0.01 s.\n",
      "2022-06-15 05:51:41,691 - base - recpack - INFO - Processed epoch 7 in 0.05 s.Batch Training Loss = 0.3989\n",
      "2022-06-15 05:51:41,703 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.9545933701454672, which is better than previous iterations.\n",
      "2022-06-15 05:51:41,704 - base - recpack - INFO - Model improved. Storing better model.\n",
      "2022-06-15 05:51:41,707 - base - recpack - INFO - Evaluation at end of 7 took 0.02 s.\n",
      "2022-06-15 05:51:41,756 - base - recpack - INFO - Processed epoch 8 in 0.05 s.Batch Training Loss = 0.3235\n",
      "2022-06-15 05:51:41,768 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.9750574842557924, which is better than previous iterations.\n",
      "2022-06-15 05:51:41,769 - base - recpack - INFO - Model improved. Storing better model.\n",
      "2022-06-15 05:51:41,772 - base - recpack - INFO - Evaluation at end of 8 took 0.01 s.\n",
      "2022-06-15 05:51:41,824 - base - recpack - INFO - Processed epoch 9 in 0.05 s.Batch Training Loss = 0.3258\n",
      "2022-06-15 05:51:41,836 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.9999999999999999, which is better than previous iterations.\n",
      "2022-06-15 05:51:41,837 - base - recpack - INFO - Model improved. Storing better model.\n",
      "2022-06-15 05:51:41,840 - base - recpack - INFO - Evaluation at end of 9 took 0.02 s.\n",
      "2022-06-15 05:51:41,890 - base - recpack - INFO - Processed epoch 10 in 0.05 s.Batch Training Loss = 0.2526\n",
      "2022-06-15 05:51:41,902 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.9489044006028783, which is worse than previous iterations.\n",
      "2022-06-15 05:51:41,903 - base - recpack - INFO - Evaluation at end of 10 took 0.01 s.\n",
      "2022-06-15 05:51:41,951 - base - recpack - INFO - Processed epoch 11 in 0.05 s.Batch Training Loss = 0.1978\n",
      "2022-06-15 05:51:41,965 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.9355245321275762, which is worse than previous iterations.\n",
      "2022-06-15 05:51:41,965 - base - recpack - INFO - Evaluation at end of 11 took 0.01 s.\n",
      "2022-06-15 05:51:42,017 - base - recpack - INFO - Processed epoch 12 in 0.05 s.Batch Training Loss = 0.1661\n",
      "2022-06-15 05:51:42,030 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.9251084237866075, which is worse than previous iterations.\n",
      "2022-06-15 05:51:42,031 - base - recpack - INFO - Evaluation at end of 12 took 0.01 s.\n",
      "2022-06-15 05:51:42,079 - base - recpack - INFO - Processed epoch 13 in 0.05 s.Batch Training Loss = 0.2142\n",
      "2022-06-15 05:51:42,092 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.9866201315246979, which is worse than previous iterations.\n",
      "2022-06-15 05:51:42,092 - base - recpack - INFO - Evaluation at end of 13 took 0.01 s.\n",
      "2022-06-15 05:51:42,139 - base - recpack - INFO - Processed epoch 14 in 0.05 s.Batch Training Loss = 0.1242\n",
      "2022-06-15 05:51:42,152 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.9251084237866075, which is worse than previous iterations.\n",
      "2022-06-15 05:51:42,152 - base - recpack - INFO - Evaluation at end of 14 took 0.01 s.\n",
      "2022-06-15 05:51:42,200 - base - recpack - INFO - Processed epoch 15 in 0.05 s.Batch Training Loss = 0.0991\n",
      "2022-06-15 05:51:42,212 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.9866201315246979, which is worse than previous iterations.\n",
      "2022-06-15 05:51:42,213 - base - recpack - INFO - Evaluation at end of 15 took 0.01 s.\n",
      "2022-06-15 05:51:42,270 - base - recpack - INFO - Processed epoch 16 in 0.06 s.Batch Training Loss = 0.1188\n",
      "2022-06-15 05:51:42,282 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.9866201315246979, which is worse than previous iterations.\n",
      "2022-06-15 05:51:42,283 - base - recpack - INFO - Evaluation at end of 16 took 0.01 s.\n",
      "2022-06-15 05:51:42,333 - base - recpack - INFO - Processed epoch 17 in 0.05 s.Batch Training Loss = 0.1588\n",
      "2022-06-15 05:51:42,345 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.9489044006028784, which is worse than previous iterations.\n",
      "2022-06-15 05:51:42,345 - base - recpack - INFO - Evaluation at end of 17 took 0.01 s.\n",
      "2022-06-15 05:51:42,397 - base - recpack - INFO - Processed epoch 18 in 0.05 s.Batch Training Loss = 0.0280\n",
      "2022-06-15 05:51:42,410 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.9866201315246979, which is worse than previous iterations.\n",
      "2022-06-15 05:51:42,411 - base - recpack - INFO - Evaluation at end of 18 took 0.01 s.\n",
      "2022-06-15 05:51:42,458 - base - recpack - INFO - Processed epoch 19 in 0.05 s.Batch Training Loss = 0.0902\n",
      "2022-06-15 05:51:42,470 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.9866201315246979, which is worse than previous iterations.\n",
      "2022-06-15 05:51:42,471 - base - recpack - INFO - Evaluation at end of 19 took 0.01 s.\n",
      "2022-06-15 05:51:42,474 - base - recpack - INFO - Fitting NeuMF complete - Took 1.27s\n"
     ]
    }
   ],
   "source": [
    "def test_overfit(mat):\n",
    "    m = NeuMF(\n",
    "        num_components=10,\n",
    "        batch_size=1,\n",
    "        max_epochs=20,\n",
    "        learning_rate=0.02,\n",
    "        stopping_criterion=\"ndcg\",\n",
    "        U=1,\n",
    "    )\n",
    "\n",
    "    # set sampler to exact sampling\n",
    "    m.sampler.exact = True\n",
    "    m.fit(mat, (mat, mat))\n",
    "    bin_mat = mat.binary_values\n",
    "    pred = m.predict(mat.binary_values).toarray()\n",
    "    for user in mat.active_users:\n",
    "        # The model should have overfitted, so that the visited items have the highest similarities\n",
    "        positives = bin_mat[user].nonzero()[1]\n",
    "        negatives = list(set(range(mat.shape[1])) - set(positives))\n",
    "\n",
    "        for item in positives:\n",
    "            assert (pred[user][negatives] < pred[user, item]).all()\n",
    "            \n",
    "test_overfit(mat)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b414610",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Experiment\n",
    "\n",
    "Use RecPack Pipeline to compare the newly implemented algorithm to frequently used baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f6e436",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Now that we have implemented a new algorithm, we want to compare its performance to some baseline algorithms.\n",
    "This acts both as a sanity check in terms of performance, as an important step to make a contribution to the field.\n",
    "\n",
    "For easy experimentation RecPack provides a pipeline setup.\n",
    "\n",
    "We’ll use the PipelineBuilder to construct our pipeline, and once constructed run it to finally get comparative results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3225456",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from recpack.pipelines import PipelineBuilder\n",
    "from recpack.data.datasets import MovieLens25M\n",
    "from recpack.splitters.scenarios import WeakGeneralization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70218f3",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We need the following components:\n",
    "\n",
    "- PipelineBuilder\n",
    "- A Dataset\n",
    "- A Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "345019f0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3584dc7b71340c294b4edc4cc0bcddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24552674 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66f3fd732cc34d5197e612322f250633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24552674 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = MovieLens25M(\n",
    "    path='/home/robinverachtert/datasets',\n",
    ")\n",
    "data = dataset.load_interaction_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "47169d75",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Subsample to 1000 users to make it faster\n",
    "# import numpy as np\n",
    "\n",
    "# users = np.random.choice(list(data.active_users), 1000)\n",
    "# data = data.users_in(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a5a8b9ab",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4be0a8bdc3ec4c28a25abaddc6541d0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56d792a3baa0401a8ebf67e7966540ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scenario = WeakGeneralization(frac_data_in=0.8, validation=True)\n",
    "scenario.split(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba6aa1bc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from recpack.pipelines import ALGORITHM_REGISTRY\n",
    "ALGORITHM_REGISTRY.register('NeuMF', NeuMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2c46caa2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "builder = PipelineBuilder()\n",
    "builder.set_data_from_scenario(scenario)\n",
    "builder.set_optimisation_metric('NormalizedDiscountedCumulativeGainK', K=10)\n",
    "builder.add_metric('NormalizedDiscountedCumulativeGainK', K=10)\n",
    "builder.add_metric('CoverageK', K=10)\n",
    "\n",
    "builder.add_algorithm(\n",
    "    algorithm = 'NeuMF', \n",
    "    params = {\n",
    "        'batch_size': 128,\n",
    "        'max_epochs': 10,\n",
    "        'learning_rate': 0.01,\n",
    "        'stopping_criterion': 'ndcg',\n",
    "        'predict_topK': 20,\n",
    "        'U': 3,\n",
    "        'dropout': 0.01\n",
    "    },\n",
    "    grid = {\n",
    "        'num_components': [16, 32, 64],\n",
    "    }\n",
    ")\n",
    "\n",
    "builder.add_algorithm('Popularity', params={'K': 20})\n",
    "builder.add_algorithm('ItemKNN', grid={'similarity': ['conditional_probability', 'cosine']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "05c0a518",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = builder.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3314c304",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0706b29894234a87afa3f13e2be1fd46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-15 06:35:24,819 - base - recpack - INFO - Processed epoch 0 in 494.97 s.Batch Training Loss = 34.3716\n",
      "2022-06-15 07:14:02,984 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.08098079182569473, which is better than previous iterations.\n",
      "2022-06-15 07:14:02,985 - base - recpack - INFO - Model improved. Storing better model.\n",
      "2022-06-15 07:14:03,006 - base - recpack - INFO - Evaluation at end of 0 took 2318.19 s.\n",
      "2022-06-15 07:22:18,172 - base - recpack - INFO - Processed epoch 1 in 495.16 s.Batch Training Loss = 29.6567\n",
      "2022-06-15 08:01:30,075 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.08498456229376744, which is better than previous iterations.\n",
      "2022-06-15 08:01:30,076 - base - recpack - INFO - Model improved. Storing better model.\n",
      "2022-06-15 08:01:30,100 - base - recpack - INFO - Evaluation at end of 1 took 2351.93 s.\n",
      "2022-06-15 08:09:48,116 - base - recpack - INFO - Processed epoch 2 in 498.01 s.Batch Training Loss = 28.6461\n",
      "2022-06-15 08:49:19,316 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.09197297670850743, which is better than previous iterations.\n",
      "2022-06-15 08:49:19,317 - base - recpack - INFO - Model improved. Storing better model.\n",
      "2022-06-15 08:49:19,340 - base - recpack - INFO - Evaluation at end of 2 took 2371.22 s.\n",
      "2022-06-15 08:57:38,037 - base - recpack - INFO - Processed epoch 3 in 498.69 s.Batch Training Loss = 28.0308\n",
      "2022-06-15 09:37:53,888 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.08559611504267729, which is worse than previous iterations.\n",
      "2022-06-15 09:37:53,889 - base - recpack - INFO - Evaluation at end of 3 took 2415.85 s.\n",
      "2022-06-15 09:46:05,265 - base - recpack - INFO - Processed epoch 4 in 491.37 s.Batch Training Loss = 27.5345\n",
      "2022-06-15 10:25:03,679 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.08759302877270317, which is worse than previous iterations.\n",
      "2022-06-15 10:25:03,680 - base - recpack - INFO - Evaluation at end of 4 took 2338.41 s.\n",
      "2022-06-15 10:33:16,393 - base - recpack - INFO - Processed epoch 5 in 492.71 s.Batch Training Loss = 27.1362\n",
      "2022-06-15 11:11:46,595 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.0874118213840824, which is worse than previous iterations.\n",
      "2022-06-15 11:11:46,596 - base - recpack - INFO - Evaluation at end of 5 took 2310.20 s.\n",
      "2022-06-15 11:19:53,901 - base - recpack - INFO - Processed epoch 6 in 487.30 s.Batch Training Loss = 26.8042\n",
      "2022-06-15 11:59:11,728 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.08561812185582184, which is worse than previous iterations.\n",
      "2022-06-15 11:59:11,729 - base - recpack - INFO - Evaluation at end of 6 took 2357.83 s.\n",
      "2022-06-15 12:07:26,814 - base - recpack - INFO - Processed epoch 7 in 495.08 s.Batch Training Loss = 26.5912\n",
      "2022-06-15 12:47:33,351 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.09090779227759263, which is worse than previous iterations.\n",
      "2022-06-15 12:47:33,353 - base - recpack - INFO - Evaluation at end of 7 took 2406.54 s.\n",
      "2022-06-15 12:55:46,148 - base - recpack - INFO - Processed epoch 8 in 492.79 s.Batch Training Loss = 26.3973\n",
      "2022-06-15 13:35:45,774 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.08397258749112704, which is worse than previous iterations.\n",
      "2022-06-15 13:35:45,776 - base - recpack - INFO - Evaluation at end of 8 took 2399.63 s.\n",
      "2022-06-15 13:43:57,982 - base - recpack - INFO - Processed epoch 9 in 492.20 s.Batch Training Loss = 26.2929\n",
      "2022-06-15 14:23:34,407 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.08976968933028731, which is worse than previous iterations.\n",
      "2022-06-15 14:23:34,409 - base - recpack - INFO - Evaluation at end of 9 took 2376.43 s.\n",
      "2022-06-15 14:23:34,421 - base - recpack - INFO - Fitting NeuMF complete - Took 2.86e+04s\n",
      "2022-06-15 15:13:42,030 - base - recpack - INFO - Processed epoch 0 in 679.16 s.Batch Training Loss = 34.8242\n",
      "2022-06-15 15:54:02,449 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.08084150294272156, which is better than previous iterations.\n",
      "2022-06-15 15:54:02,449 - base - recpack - INFO - Model improved. Storing better model.\n",
      "2022-06-15 15:54:02,489 - base - recpack - INFO - Evaluation at end of 0 took 2420.46 s.\n",
      "2022-06-15 16:05:23,944 - base - recpack - INFO - Processed epoch 1 in 681.45 s.Batch Training Loss = 30.8581\n",
      "2022-06-15 16:35:56,356 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.08315575480126193, which is better than previous iterations.\n",
      "2022-06-15 16:35:56,357 - base - recpack - INFO - Model improved. Storing better model.\n",
      "2022-06-15 16:35:56,400 - base - recpack - INFO - Evaluation at end of 1 took 1832.46 s.\n",
      "2022-06-15 16:47:19,309 - base - recpack - INFO - Processed epoch 2 in 682.90 s.Batch Training Loss = 31.6647\n",
      "2022-06-15 17:17:39,531 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.08496767842023471, which is better than previous iterations.\n",
      "2022-06-15 17:17:39,532 - base - recpack - INFO - Model improved. Storing better model.\n",
      "2022-06-15 17:17:39,573 - base - recpack - INFO - Evaluation at end of 2 took 1820.26 s.\n",
      "2022-06-15 17:29:01,841 - base - recpack - INFO - Processed epoch 3 in 682.26 s.Batch Training Loss = 31.5660\n",
      "2022-06-15 17:59:12,038 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.08968784738356625, which is better than previous iterations.\n",
      "2022-06-15 17:59:12,039 - base - recpack - INFO - Model improved. Storing better model.\n",
      "2022-06-15 17:59:12,083 - base - recpack - INFO - Evaluation at end of 3 took 1810.24 s.\n",
      "2022-06-15 18:10:30,675 - base - recpack - INFO - Processed epoch 4 in 678.58 s.Batch Training Loss = 32.0441\n",
      "2022-06-15 18:39:49,027 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.0855364542911806, which is worse than previous iterations.\n",
      "2022-06-15 18:39:49,029 - base - recpack - INFO - Evaluation at end of 4 took 1758.35 s.\n",
      "2022-06-15 18:51:04,234 - base - recpack - INFO - Processed epoch 5 in 675.20 s.Batch Training Loss = 41.8893\n",
      "2022-06-15 19:20:08,628 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.08509129482194867, which is worse than previous iterations.\n",
      "2022-06-15 19:20:08,629 - base - recpack - INFO - Evaluation at end of 5 took 1744.39 s.\n",
      "2022-06-15 19:31:19,002 - base - recpack - INFO - Processed epoch 6 in 670.37 s.Batch Training Loss = 32.3746\n",
      "2022-06-15 20:00:42,291 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.027030131162751485, which is worse than previous iterations.\n",
      "2022-06-15 20:00:42,293 - base - recpack - INFO - Evaluation at end of 6 took 1763.29 s.\n",
      "2022-06-15 20:11:55,715 - base - recpack - INFO - Processed epoch 7 in 673.41 s.Batch Training Loss = 34.9569\n",
      "2022-06-15 20:41:11,758 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.020750219545088277, which is worse than previous iterations.\n",
      "2022-06-15 20:41:11,760 - base - recpack - INFO - Evaluation at end of 7 took 1756.04 s.\n",
      "2022-06-15 20:52:24,313 - base - recpack - INFO - Processed epoch 8 in 672.55 s.Batch Training Loss = 36.4821\n",
      "2022-06-15 21:21:53,264 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.012300467698771009, which is worse than previous iterations.\n",
      "2022-06-15 21:21:53,266 - base - recpack - INFO - Evaluation at end of 8 took 1768.95 s.\n",
      "2022-06-15 21:33:06,922 - base - recpack - INFO - Processed epoch 9 in 673.65 s.Batch Training Loss = 40.5475\n",
      "2022-06-15 22:02:24,372 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.0128943838126462, which is worse than previous iterations.\n",
      "2022-06-15 22:02:24,374 - base - recpack - INFO - Evaluation at end of 9 took 1757.45 s.\n",
      "2022-06-15 22:02:24,393 - base - recpack - INFO - Fitting NeuMF complete - Took 2.52e+04s\n",
      "2022-06-15 22:47:50,536 - base - recpack - INFO - Processed epoch 0 in 1033.30 s.Batch Training Loss = 36.5534\n",
      "2022-06-15 23:19:32,433 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.08207414461287382, which is better than previous iterations.\n",
      "2022-06-15 23:19:32,434 - base - recpack - INFO - Model improved. Storing better model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-15 23:19:32,512 - base - recpack - INFO - Evaluation at end of 0 took 1901.97 s.\n",
      "2022-06-15 23:36:46,288 - base - recpack - INFO - Processed epoch 1 in 1033.77 s.Batch Training Loss = 34.8841\n",
      "2022-06-16 00:08:22,144 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.016820435356538987, which is worse than previous iterations.\n",
      "2022-06-16 00:08:22,145 - base - recpack - INFO - Evaluation at end of 1 took 1895.86 s.\n",
      "2022-06-16 00:25:33,703 - base - recpack - INFO - Processed epoch 2 in 1031.55 s.Batch Training Loss = 49.3086\n",
      "2022-06-16 00:57:12,222 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.019952507258523974, which is worse than previous iterations.\n",
      "2022-06-16 00:57:12,224 - base - recpack - INFO - Evaluation at end of 2 took 1898.52 s.\n",
      "2022-06-16 01:14:27,522 - base - recpack - INFO - Processed epoch 3 in 1035.29 s.Batch Training Loss = 43.7914\n",
      "2022-06-16 01:45:57,922 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.013950901586331336, which is worse than previous iterations.\n",
      "2022-06-16 01:45:57,923 - base - recpack - INFO - Evaluation at end of 3 took 1890.40 s.\n",
      "2022-06-16 02:03:13,035 - base - recpack - INFO - Processed epoch 4 in 1035.10 s.Batch Training Loss = 66.9058\n",
      "2022-06-16 02:34:47,315 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.010873944142251266, which is worse than previous iterations.\n",
      "2022-06-16 02:34:47,316 - base - recpack - INFO - Evaluation at end of 4 took 1894.28 s.\n",
      "2022-06-16 02:52:03,319 - base - recpack - INFO - Processed epoch 5 in 1035.99 s.Batch Training Loss = 42.2020\n",
      "2022-06-16 03:23:41,896 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.009876389680190794, which is worse than previous iterations.\n",
      "2022-06-16 03:23:41,898 - base - recpack - INFO - Evaluation at end of 5 took 1898.58 s.\n",
      "2022-06-16 03:40:58,706 - base - recpack - INFO - Processed epoch 6 in 1036.80 s.Batch Training Loss = 41.6841\n",
      "2022-06-16 04:12:34,179 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.012165661767016363, which is worse than previous iterations.\n",
      "2022-06-16 04:12:34,180 - base - recpack - INFO - Evaluation at end of 6 took 1895.47 s.\n",
      "2022-06-16 04:29:45,871 - base - recpack - INFO - Processed epoch 7 in 1031.68 s.Batch Training Loss = 40.9009\n",
      "2022-06-16 05:02:08,998 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.0071517136142810874, which is worse than previous iterations.\n",
      "2022-06-16 05:02:08,999 - base - recpack - INFO - Evaluation at end of 7 took 1943.13 s.\n",
      "2022-06-16 05:19:22,014 - base - recpack - INFO - Processed epoch 8 in 1033.01 s.Batch Training Loss = 45.5613\n",
      "2022-06-16 05:51:15,520 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.013573717408627416, which is worse than previous iterations.\n",
      "2022-06-16 05:51:15,521 - base - recpack - INFO - Evaluation at end of 8 took 1913.51 s.\n",
      "2022-06-16 06:08:27,854 - base - recpack - INFO - Processed epoch 9 in 1032.33 s.Batch Training Loss = 41.8714\n",
      "2022-06-16 06:39:49,045 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.008489497092389687, which is worse than previous iterations.\n",
      "2022-06-16 06:39:49,047 - base - recpack - INFO - Evaluation at end of 9 took 1881.19 s.\n",
      "2022-06-16 06:39:49,079 - base - recpack - INFO - Fitting NeuMF complete - Took 2.94e+04s\n",
      "2022-06-16 07:19:28,070 - base - recpack - INFO - Processed epoch 0 in 483.40 s.Batch Training Loss = 35.1290\n",
      "2022-06-16 07:58:15,199 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.034901703510858084, which is better than previous iterations.\n",
      "2022-06-16 07:58:15,200 - base - recpack - INFO - Model improved. Storing better model.\n",
      "2022-06-16 07:58:15,221 - base - recpack - INFO - Evaluation at end of 0 took 2327.15 s.\n",
      "2022-06-16 08:06:22,251 - base - recpack - INFO - Processed epoch 1 in 487.02 s.Batch Training Loss = 30.3454\n",
      "2022-06-16 08:45:05,813 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.03205059813936343, which is worse than previous iterations.\n",
      "2022-06-16 08:45:05,815 - base - recpack - INFO - Evaluation at end of 1 took 2323.56 s.\n",
      "2022-06-16 08:53:15,278 - base - recpack - INFO - Processed epoch 2 in 489.46 s.Batch Training Loss = 29.1253\n",
      "2022-06-16 09:32:18,997 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.030603384690768374, which is worse than previous iterations.\n",
      "2022-06-16 09:32:18,998 - base - recpack - INFO - Evaluation at end of 2 took 2343.72 s.\n",
      "2022-06-16 09:40:28,420 - base - recpack - INFO - Processed epoch 3 in 489.41 s.Batch Training Loss = 28.4542\n",
      "2022-06-16 10:19:42,862 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.031809037429649176, which is worse than previous iterations.\n",
      "2022-06-16 10:19:42,864 - base - recpack - INFO - Evaluation at end of 3 took 2354.44 s.\n",
      "2022-06-16 10:27:51,606 - base - recpack - INFO - Processed epoch 4 in 488.73 s.Batch Training Loss = 28.0836\n",
      "2022-06-16 11:06:03,599 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.026874878989349225, which is worse than previous iterations.\n",
      "2022-06-16 11:06:03,600 - base - recpack - INFO - Evaluation at end of 4 took 2291.99 s.\n",
      "2022-06-16 11:14:08,516 - base - recpack - INFO - Processed epoch 5 in 484.91 s.Batch Training Loss = 27.7254\n",
      "2022-06-16 11:53:00,221 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.026679740427533398, which is worse than previous iterations.\n",
      "2022-06-16 11:53:00,223 - base - recpack - INFO - Evaluation at end of 5 took 2331.71 s.\n",
      "2022-06-16 12:01:12,617 - base - recpack - INFO - Processed epoch 6 in 492.39 s.Batch Training Loss = 27.4668\n",
      "2022-06-16 12:38:56,334 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.028190025391976795, which is worse than previous iterations.\n",
      "2022-06-16 12:38:56,336 - base - recpack - INFO - Evaluation at end of 6 took 2263.72 s.\n",
      "2022-06-16 12:46:53,481 - base - recpack - INFO - Processed epoch 7 in 477.14 s.Batch Training Loss = 27.2134\n",
      "2022-06-16 13:24:31,906 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.02698406660372085, which is worse than previous iterations.\n",
      "2022-06-16 13:24:31,907 - base - recpack - INFO - Evaluation at end of 7 took 2258.43 s.\n",
      "2022-06-16 13:32:34,363 - base - recpack - INFO - Processed epoch 8 in 482.45 s.Batch Training Loss = 27.0140\n",
      "2022-06-16 14:10:28,524 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.029020783139315873, which is worse than previous iterations.\n",
      "2022-06-16 14:10:28,526 - base - recpack - INFO - Evaluation at end of 8 took 2274.16 s.\n",
      "2022-06-16 14:18:32,604 - base - recpack - INFO - Processed epoch 9 in 484.07 s.Batch Training Loss = 26.8622\n",
      "2022-06-16 14:56:16,971 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.02755323275195566, which is worse than previous iterations.\n",
      "2022-06-16 14:56:16,973 - base - recpack - INFO - Evaluation at end of 9 took 2264.37 s.\n",
      "2022-06-16 14:56:16,986 - base - recpack - INFO - Fitting NeuMF complete - Took 2.79e+04s\n",
      "2022-06-16 15:33:52,402 - base - recpack - INFO - Fitting Popularity complete - Took 5.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robinverachtert/dist-env/lib/python3.8/site-packages/scipy/sparse/_index.py:146: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-16 15:35:19,657 - base - recpack - INFO - Fitting ItemKNN complete - Took 75.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robinverachtert/dist-env/lib/python3.8/site-packages/recpack/algorithms/base.py:274: UserWarning: ItemKNN missing similar items for 1 items.\n",
      "  warnings.warn(f\"{self.name} missing similar items for {missing} items.\")\n",
      "/home/robinverachtert/dist-env/lib/python3.8/site-packages/scipy/sparse/_index.py:146: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-16 15:36:40,274 - base - recpack - INFO - Fitting ItemKNN complete - Took 55.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robinverachtert/dist-env/lib/python3.8/site-packages/recpack/algorithms/base.py:274: UserWarning: ItemKNN missing similar items for 1 items.\n",
      "  warnings.warn(f\"{self.name} missing similar items for {missing} items.\")\n",
      "/home/robinverachtert/dist-env/lib/python3.8/site-packages/scipy/sparse/_index.py:146: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-16 15:38:30,229 - base - recpack - INFO - Fitting ItemKNN complete - Took 56.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robinverachtert/dist-env/lib/python3.8/site-packages/recpack/algorithms/base.py:274: UserWarning: ItemKNN missing similar items for 1 items.\n",
      "  warnings.warn(f\"{self.name} missing similar items for {missing} items.\")\n"
     ]
    }
   ],
   "source": [
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8af63fb1",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3a36b1d0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>normalizeddiscountedcumulativegaink_10</th>\n",
       "      <th>coveragek_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NeuMF(U=3,batch_size=128,dropout=0.01,exact_sampling=False,keep_last=False,learning_rate=0.01,max_epochs=10,max_iter_no_change=5,min_improvement=0.0,num_components=16,predict_topK=20,save_best_to_file=False,seed=2404017312,stop_early=False,stopping_criterion=&lt;recpack.algorithms.stopping_criterion.StoppingCriterion object at 0x7fa205cd3c40&gt;)</th>\n",
       "      <td>0.045246</td>\n",
       "      <td>0.038300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Popularity(K=20)</th>\n",
       "      <td>0.094183</td>\n",
       "      <td>0.000313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ItemKNN(K=200,normalize=False,normalize_X=False,normalize_sim=False,pop_discount=None,similarity=cosine)</th>\n",
       "      <td>0.153369</td>\n",
       "      <td>0.083840</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    normalizeddiscountedcumulativegaink_10  coveragek_10\n",
       "NeuMF(U=3,batch_size=128,dropout=0.01,exact_sam...                                0.045246      0.038300\n",
       "Popularity(K=20)                                                                  0.094183      0.000313\n",
       "ItemKNN(K=200,normalize=False,normalize_X=False...                                0.153369      0.083840"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(pipeline.get_metrics()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720b3479",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbe6c68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a929f25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f6424f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa99149",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c6391a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4211ea3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84adbb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51fe3f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcc4d5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db3b2d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "7da6e734",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
