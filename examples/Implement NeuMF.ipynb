{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85c37828",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pytest\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from typing import Callable, List, Optional, Union"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649de821",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RecPack: An Experimental Toolkit for Recommendation Algorithms\n",
    "\n",
    "by Lien Michiels and Robin Verachtert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018ba03f",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Welcome to this demo of the RecPack experimentation framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dca131",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is Recpack?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478837d4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What is Recpack?\n",
    "- Python library for recommendation algorithm implementation and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf1660c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What is Recpack?\n",
    "- Python library for recommendation algorithm implementation and evaluation\n",
    "- Focus on being easily extendable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d592e03",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Extendable :\n",
    "- Makes it easy for researchers to compare their new algorithms to the baselines.\n",
    "- Loosely coopled modules makes for a (shallow?) learning curve to add your own components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8cbaac",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What is Recpack?\n",
    "### Modules and interactions\n",
    "![pipeline](RecpackPipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aad33e9",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "TODO: Data -> datasets\n",
    "    Pre-filters -> preprocessing\n",
    "    Post-filters -> postprocessing\n",
    "    Algorithm -> Algorithms\n",
    "    \n",
    "Boring picture?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab307a0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## This Demo\n",
    "- Demonstrate implementation of new algorithm using \"Neural Matrix Factorization (He et al. 2017)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ffa876",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In order to show the user how to use the package, and what we mean with extendable, we show how to implement a Neural Network using pytorch.\n",
    "Thanks to a functionality richt baseclass this requires limited implementation, and thus facilitates faster development."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c6c8a6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## This Demo\n",
    "- Demonstrate implementation of new algorithm using \"Neural Matrix Factorization (He et al. 2017)\"\n",
    "- Run experiment to compare performance of new algorithm to baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625bf0bf",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Once we have implemented the algorithm, we throw it into the arena against some baselines.\n",
    "Showcasing the pipeline functionality that acts as glue between the different recpack components as well as providing an intuitive way to create and run experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d08a2c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Matrix Factorization\n",
    "- Users and items are represented by embedding fectors\n",
    "- Similarity is modeled using an MLP network, rather than computing <u,i> as in traditional matrix factorization embedding techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b65d3d8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Architecture](NeuMFArchitecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028dde0d",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "For simplicity, and to focus on the recpack functionality, we implement the NMF approach that only uses an MLP, as presented in Figure 2 in the original paper.\n",
    "\n",
    "The architecture boils down to 2 embeddings, an MLP module and a final output conversion function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c196de69",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Implementing MLP network\n",
    "\n",
    "![MLPArchitecture](MLPArchitecture.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05742407",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In order to make the MLP reusable we will implement it as its own Torch module.\n",
    "The parameters include: layer dimensions for each layer, activation function to be used after each linear operation and the number of output nodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "743b93a9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Code used from https://github.com/facebookresearch/multimodal/blob/5dec8a/torchmultimodal/modules/layers/mlp.py\n",
    "# Another option is to use torchvision.ops.MLP \n",
    "# which is nearly identical in implementation, but is in torchvision and not in base torch.\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"A multi-layer perceptron module.\n",
    "    This module is a sequence of linear layers plus activation functions.\n",
    "    The user can optionally add normalization and/or dropout to each of the layers.\n",
    "    \n",
    "    Code used from https://github.com/facebookresearch/multimodal/blob/5dec8a/torchmultimodal/modules/layers/mlp.py\n",
    "    \n",
    "    :param in_dim: Input dimension.\n",
    "    :type in_dim: int\n",
    "    :param out_dim: Output dimension.\n",
    "    :type out_dim: int\n",
    "    :param hidden_dims: Output dimension for each hidden layer.\n",
    "    :type hidden_dims: Optional[Union[int, List[int]]] \n",
    "    :param dropout: Probability for dropout layers between each hidden layer.\n",
    "    :type dropout: float\n",
    "    :param activation: Which activation function to use. \n",
    "        Supports module type or partial.\n",
    "    :type activation: Callable[..., nn.Module]\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfebfee3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class MLP(MLP):\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_dim: int, \n",
    "        out_dim: int,\n",
    "        hidden_dims: Optional[Union[int, List[int]]] = None,\n",
    "        dropout: float = 0.5,\n",
    "        activation: Callable[..., nn.Module] = nn.ReLU,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = nn.ModuleList()\n",
    "\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = []\n",
    "\n",
    "        if isinstance(hidden_dims, int):\n",
    "            hidden_dims = [hidden_dims]\n",
    "\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(in_dim, hidden_dim))\n",
    "            layers.append(activation())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            in_dim = hidden_dim\n",
    "        layers.append(nn.Linear(in_dim, out_dim))\n",
    "        self.model = nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "353239bc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class MLP(MLP):\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83fcd62d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 3\n",
    "N_USERS = 5\n",
    "N_ITEMS = 10\n",
    "a = MLP(2*EMBEDDING_SIZE, N_ITEMS, [6, 4, 2], dropout=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb2a3f98",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: add tests that check that the MLP works as expected.\n",
    "def test_MLP_shapes():\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea26454",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Implementing the Neural Architecture\n",
    "\n",
    "![Architecture](NeuMFArchitecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b8871a",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Given the MLP module implemented before, our NeuMF module includes 2 embeddings, 1 mlp block and a final activation function to modify the scores to the 0-1 interval\n",
    "\n",
    "We also need to define the interface for the forward function.\n",
    "Since we need both a user_id and an item_id, it makes sense to have two parameters: user and items\n",
    "Both are 1D tensors of size L, that contain ids\n",
    "\n",
    "When calling forward the embeddings will be looked up for each user and item, generating two 2D tensors\n",
    "These are concatenated (horizontally) into a single matrix of dimensions L x num_components\n",
    "\n",
    "The concatenated matrix is passed through the MLP, which will predict a final score for each u,i pair.\n",
    "These scores are put into the 0, 1 interval through the use of a sigmoid function.\n",
    "\n",
    "And this is returned. Comparison to targets, and learning will happen in the baseclass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34a78b55",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class NMFModule(nn.Module):\n",
    "    \"\"\"Model that encodes the Neural Matrix Factorization Network.\n",
    "    \n",
    "    Implements the 3 tiered network defined in the He et al. paper.\n",
    "\n",
    "    :param predictive_powers: size of the last hidden layer in MLP.\n",
    "        Embedding sizes computed as 2 * predictive powers.\n",
    "    :type predictive_powers: int\n",
    "    :param n_users: number of users in the network\n",
    "    :type n_users: int\n",
    "    :param n_items: number of items in the network\n",
    "    :type n_items: int\n",
    "    :param hidden_dims: dimensions of the MLP hidden layers.\n",
    "    :type hidden_dims: Union[int, List[int]]\n",
    "    :param dropout: Dropout chance between layers of the MLP\n",
    "    :type dropout: float\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63e439d5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class NMFModule(NMFModule):\n",
    "    def __init__(\n",
    "        self, predictive_powers: int, n_users: int, n_items: int, dropout: float\n",
    "    ):\n",
    "        super().__init__()\n",
    "        num_components = 2 * predictive_powers\n",
    "        \n",
    "        self.user_embedding = nn.Embedding(n_users, num_components)\n",
    "        self.item_embedding = nn.Embedding(n_items, num_components)\n",
    "\n",
    "        # we use a three tiered MLP as described in the experiments of the paper.\n",
    "        hidden_dims = [\n",
    "            4 * predictive_powers, \n",
    "            2 * predictive_powers, \n",
    "            predictive_powers\n",
    "        ]\n",
    "\n",
    "        # Output is always 1, since we need a single score for u,i\n",
    "        self.mlp = MLP(4 * predictive_powers, 1, \n",
    "                       hidden_dims, dropout=dropout)\n",
    "\n",
    "        self.final = nn.Sigmoid()\n",
    "\n",
    "        # weight initialization\n",
    "        self.user_embedding.weight.data.normal_(0, \n",
    "            1.0 / self.user_embedding.embedding_dim)\n",
    "        self.item_embedding.weight.data.normal_(0, \n",
    "            1.0 / self.item_embedding.embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "551d5c5b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class NMFModule(NMFModule):\n",
    "    def forward(self, users: torch.LongTensor, items: torch.LongTensor) -> torch.FloatTensor:\n",
    "        \"\"\"Predict scores for the user item pairs obtained \n",
    "        by zipping together the two inputs\n",
    "\n",
    "        :param users: 1D tensor with user ids\n",
    "        :type users: torch.LongTensor\n",
    "        :param items: 1D tensor with item ids\n",
    "        :type items: torch.LongTensor\n",
    "        :return: 1D tensor with predicted similarities.\n",
    "            Position i is the similarity between \n",
    "            `users[i]` and `items[i]`\n",
    "        :rtype: torch.FloatTensor\n",
    "        \"\"\"\n",
    "\n",
    "        # Embedding lookups\n",
    "        user_emb = self.user_embedding(users)\n",
    "        item_emb = self.item_embedding(items)\n",
    "\n",
    "        # Pass concatenated through MLP and apply sigmoid\n",
    "        return self.final(\n",
    "            self.mlp(\n",
    "                torch.hstack([user_emb, item_emb])\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8bcb491",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def test_output_shapes_NMF(\n",
    "    predictive_factors, num_users, num_items\n",
    "):\n",
    "    \"\"\"Check that no mather the inner settings of the network, the output is always correct\"\"\"\n",
    "    mod = NMFModule(predictive_factors, num_users, num_items, 0.0)\n",
    "    \n",
    "    user_tensor = torch.LongTensor([1, 2])\n",
    "    item_tensor = torch.LongTensor([1, 2])\n",
    "    \n",
    "    res = mod(user_tensor, item_tensor) # predict scores for items given the users\n",
    "    \n",
    "    assert res.shape == (2, 1)\n",
    "\n",
    "    assert (res.detach().numpy() <= 1).all()\n",
    "    assert (res.detach().numpy() >= 0).all()\n",
    "\n",
    "\n",
    "test_output_shapes_NMF(5, 10, 10)\n",
    "test_output_shapes_NMF(5, 3, 10)\n",
    "test_output_shapes_NMF(1, 3, 3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c07d570",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Union, Optional\n",
    "\n",
    "import pandas as pd\n",
    "from recpack.algorithms.base import TorchMLAlgorithm\n",
    "from recpack.algorithms.samplers import PositiveNegativeSampler\n",
    "from recpack.algorithms.util import get_users\n",
    "from recpack.matrix import InteractionMatrix\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217f8d30",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Implementing the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689f9c7d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Choosing the right baseclass\n",
    "`Algorithm`\n",
    "\n",
    "Follows the sklearn interface\n",
    "- `__init__`: sets the (hyper)parameters of the algorithm\n",
    "- `fit`: train the algorithm, and build a model that can be used for prediction\n",
    "- `predict`: Given a matrix of user histories, recommend items for these users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f889de2",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "TODO: explain Algorithm\n",
    "    \n",
    "You can overwrite these base functions, but it is advised to overload `_fit` and `_predict`, because the `fit` and `predict` contain some handy shared functionality.\n",
    "Such as:\n",
    "- timing of training\n",
    "- conversion of input to csr (can be overloaded if timestamps are needed, or InteractionMatrix is needed for another reason)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d79dab",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Choosing the right baseclass\n",
    "`TorchMLAlgorithm`\n",
    "- Implements common functionality\n",
    "    - Fitting:\n",
    "        - Epoch loop\n",
    "        - Early stopping\n",
    "        - Keep best/last model\n",
    "        - Progress logs\n",
    "    - Prediction\n",
    "        - Batched prediction loop\n",
    "        - Prune recommendations \n",
    "    - Saving + loading\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc198a66",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We could start from the Algorithm baseclass, and implement the whole learning loop and prediction from scratch.\n",
    "However RecPack provides a more specific baseclass that facilitates implementations of torch algorithms.\n",
    "\n",
    "It provides additional functionality on top of the `Algorithm` baseclass. The fit function has been implemented such that it already does a loop over the selected number of epochs. Evaluation after each epochs and checking which model to keep is also implemented.\n",
    "\n",
    "For prediction a loop over the active users in batches is implemented, recommendations are requested per batch, and then pruned.\n",
    "This way we never need to store a dense user x item matrix.\n",
    "\n",
    "Finally the baseclass also provides saving and loading functionality, which can be handy while prototyping and is also used to keep the best model at the end of the training if so desired."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef306ba",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Choosing the right baseclass\n",
    "We need to implement:\n",
    "\n",
    "- `__init__`: Sets the hyperparameters\n",
    "- `_init_model`: Initialises the model during training\n",
    "- `_train_epoch`: train for a single epoch\n",
    "- `predict_batch`: predict for a batch of users\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c56bef",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Given the baseclass, we need to implement 4 functions:\n",
    "- init: Here we need to define the additional parameters introduced by this algorithm, as well as the standard ones expected by the baseclass\n",
    "- `_init_model`: This function will be called at the start of `fit` to construct the neural network itself based on the input matrix. So in this function we need to make sure the model is constructed, and initialised correctly.\n",
    "- `_train_epoch`: While the baseclass contains a lot of functionality, we still need to implement how to perform training within a single epoch. \n",
    "- `batch_predict`: Final function to implement should perform recommendation for a select set of users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e359a201",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class NeuMF(TorchMLAlgorithm):\n",
    "    \"\"\"Implementation of Neural Matrix Factoration.\n",
    "\n",
    "    Neural Matrix Factorization based on MLP architecture\n",
    "    as presented in Figure 2 in He, Xiangnan, et al. \n",
    "    \"Neural collaborative filtering.\"\n",
    "    In Proceedings of the 26th international conference on world wide web. 2017.\n",
    "\n",
    "    Represents the users and items using an embedding, \n",
    "    similarity between the two is modelled using a neural network.\n",
    "\n",
    "    The network consists of an embedding for both users and items.\n",
    "    To compute similarity those two embeddings are \n",
    "    concatenated and passed through the MLP\n",
    "    Finally the similarity is transformed to the [0,1] domain\n",
    "    using a sigmoid function.\n",
    "\n",
    "    As in the paper, the sum of square errors is used as loss function.\n",
    "    Positive items should get a prediction close to 1, \n",
    "    while sampled negatives should get a value close to 0.\n",
    "\n",
    "    The MLP has 3 layers, as suggested in the experiments section.\n",
    "    Bottom layer has dimension `4 * predictive_powers`, \n",
    "    middle layer `2 * predictive_powers`\n",
    "    and the top layer has `predictive_powers`.\n",
    "\n",
    "    :param predictive_powers: Size of the last hidden layer in the MLP network.\n",
    "        Embedding size is 2 * predictive_powers\n",
    "    :type predictive_powers: int\n",
    "    :param batch_size: How many samples to use in each update step.\n",
    "        Higher batch sizes make each epoch more efficient,\n",
    "        but increases the amount of epochs needed to converge to the optimum,\n",
    "        by reducing the amount of updates per epoch.\n",
    "        Defaults to 512.\n",
    "    :type batch_size: Optional[int]\n",
    "    :param max_epochs: The max number of epochs to train.\n",
    "        If the stopping criterion uses early stopping, less epochs could be used.\n",
    "        Defaults to 10.\n",
    "    :type max_epochs: Optional[int]\n",
    "    :param learning_rate: How much to update the weights at each update. Defaults to 0.01\n",
    "    :type learning_rate: Optional[float]\n",
    "    :param stopping_criterion: Name of the stopping criterion to use for training.\n",
    "        For available values,\n",
    "        check :meth:`recpack.algorithms.stopping_criterion.StoppingCriterion.FUNCTIONS`\n",
    "        Defaults to 'ndcg'\n",
    "    :type stopping_criterion: Optional[str]\n",
    "    :param stop_early: If True, early stopping is enabled,\n",
    "        and after ``max_iter_no_change`` iterations where improvement of loss function\n",
    "        is below ``min_improvement`` the optimisation is stopped,\n",
    "        even if max_epochs is not reached.\n",
    "        Defaults to False\n",
    "    :type stop_early: bool, optional\n",
    "    :param max_iter_no_change: If early stopping is enabled,\n",
    "        stop after this amount of iterations without change.\n",
    "        Defaults to 5\n",
    "    :type max_iter_no_change: int, optional\n",
    "    :param min_improvement: If early stopping is enabled, no change is detected,\n",
    "        if the improvement is below this value.\n",
    "        Defaults to 0.01\n",
    "    :type min_improvement: float, optional\n",
    "    :param seed: Seed to the randomizers, useful for reproducible results,\n",
    "        defaults to None\n",
    "    :type seed: int, optional\n",
    "    :param save_best_to_file: If true, the best model will be saved after training,\n",
    "        defaults to False\n",
    "    :type save_best_to_file: bool, optional\n",
    "    :param keep_last: Retain last model, rather than best\n",
    "        (according to stopping criterion value on validation data), defaults to False\n",
    "    :type keep_last: bool, optional\n",
    "    :param predict_topK: The topK recommendations to keep per row in the matrix.\n",
    "        Use when the user x item output matrix would become too large for RAM.\n",
    "        Defaults to None, which results in no filtering.\n",
    "    :type predict_topK: int, optional\n",
    "    :param n_negatives_per_positive: Amount of negatives to sample for each positive example, defaults to 1\n",
    "    :type n_negatives_per_positive: int, optional\n",
    "    :param dropout: Dropout parameter used in MLP, defaults to 0.0\n",
    "    :type dropout: float, optional\n",
    "    :param exact_sampling: Enable or disable exact checks while sampling. \n",
    "        With exact sampling the sampled negatives are guaranteed to not have been visited by the user. \n",
    "        Non exact sampling assumes that the space for item selection is large enough, \n",
    "        such that most items are likely not seen before.\n",
    "        Defaults to False,\n",
    "    :type exact_sampling: bool, optional\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f85a3147",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class NeuMF(NeuMF):\n",
    "    def __init__(\n",
    "        self,\n",
    "        predictive_factors: int,\n",
    "        batch_size: Optional[int] = 512,\n",
    "        max_epochs: Optional[int] = 10,\n",
    "        learning_rate: Optional[float] = 0.01,\n",
    "        stopping_criterion: Optional[str] = \"ndcg\",\n",
    "        stop_early: Optional[bool] = False,\n",
    "        max_iter_no_change: Optional[int] = 5,\n",
    "        min_improvement: Optional[float] = 0.0,\n",
    "        seed: Optional[int] = None,\n",
    "        save_best_to_file: Optional[bool] = False,\n",
    "        keep_last: Optional[bool] = False,\n",
    "        predict_topK: Optional[int] = None,\n",
    "        n_negatives_per_positive: Optional[int] = 1,\n",
    "        exact_sampling: Optional[bool] = False,\n",
    "        dropout: Optional[float] = 0.0,\n",
    "    ):\n",
    "        print(batch_size, max_epochs, learning_rate, stopping_criterion)\n",
    "        super().__init__(batch_size, max_epochs, learning_rate,\n",
    "            stopping_criterion, stop_early, max_iter_no_change,\n",
    "            min_improvement, seed, save_best_to_file, keep_last,\n",
    "            predict_topK,\n",
    "        )\n",
    "\n",
    "        self.predictive_factors = predictive_factors\n",
    "\n",
    "        self.n_negatives_per_positive = n_negatives_per_positive\n",
    "        self.dropout = dropout\n",
    "        self.exact_sampling = exact_sampling\n",
    "\n",
    "        self.sampler = PositiveNegativeSampler(\n",
    "            U=self.n_negatives_per_positive, replace=False, exact=exact_sampling, \n",
    "            batch_size=self.batch_size\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152a8e1f",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In choosing our parameters we go for simple is better. We could let the user configure the structure of the MLP entirely, but then we would need to babysit them to make sure they don't make mistakes in this configuration.\n",
    "\n",
    "\n",
    "Instead we follow the authors in their experiment, and use a single parameter to define the structure of the MLP: __predictive factors__.\n",
    "This is the size of the final hidden layer in the MLP. The second layer is twice as big, while the bottom layer is twice as big again.\n",
    "\n",
    "The embedding size is 2 x the predictive factors as well, since they will be concatenated and passed to the bottom layer of the MLP, this way those two are also guaranteed equal.\n",
    "\n",
    "We also allow the user to specify the dropout parameter of the MLP architecture. We don't expose the other parameters for simplicity, and keep them at reasonable defaults.\n",
    "\n",
    "`n_negatives_per_positive`, the number of negatives to sample when training the model. For each positive, U negatives will be sampled.\n",
    "Related to sampling we also added the exact_sampling boolean. For datasets with enough items, we can assume that a random item selected is unlikely to be interacted with, and so the sampler only checks against the positive example for which a negative is sampled. For small datasets it can be advisable to set exact_sampling to True, such that the sampler checks the negatives against each of the items the user has interacted with. This way you avoid asking the network to learn opposing information.\n",
    "Exact sampling is a lot slower than approximate sampling, espcially for large datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7002850",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class NeuMF(NeuMF):\n",
    "    def _init_model(self, X: csr_matrix):\n",
    "        num_users, num_items = X.shape\n",
    "        self.model_ = NMFModule(self.predictive_factors, num_users, num_items, self.dropout).to(\n",
    "            self.device\n",
    "        )\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.model_.parameters(), lr=self.learning_rate\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7eee26b",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In the init model function we construct our NMFModule given the hyper parameters and the shape of the training matrix.\n",
    "\n",
    "We also create our optimiser, using the ADAM optimiser as suggested in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee655a20",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class NeuMF(NeuMF):\n",
    "    def _train_epoch(self, X: csr_matrix) -> List[int]:\n",
    "        losses = []\n",
    "        for users, positives, negatives in self.sampler.sample(X):\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Predict for the positives\n",
    "            positive_scores = self.model_.forward(\n",
    "                users.to(self.device), positives.to(self.device))\n",
    "            # Predict for the negatives\n",
    "            negative_scores = self.model_.forward(\n",
    "                *self._construct_negative_prediction_input(\n",
    "                    users.to(self.device), negatives.to(self.device))\n",
    "            )\n",
    "\n",
    "            loss = self._compute_loss(\n",
    "                positive_scores, negative_scores)\n",
    "\n",
    "            # Backwards propagation of the loss\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        return losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c709c0ee",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "During a single epoch the algorithm will loop in batch sizes through the positives in random order. For each positive user, item pair also U negatives will be sampled. \n",
    "\n",
    "This is all handled by a RecPack preimplemented Sampler: PositiveNegativeSampler\n",
    "\n",
    "<!-- TODO: Add slide with docs of the sampler -->\n",
    "\n",
    "For each batch we will\n",
    "compute similarities between the user u and the sampled positives and negatives given the 1D tensors for users and positives, and the 2D tensors for negatives.\n",
    "\n",
    "For the positives this is straightforward since the tensors are already in the right format.\n",
    "For the negatives we need to restructure the 2D tensor into a 1D tensor\n",
    "\n",
    "Once we have computed similarities for both positives and negatives, we can compute the loss, which we will implement next.\n",
    "\n",
    "Given the loss we let torch handle the update to the network by back propagation.\n",
    "\n",
    "We finally store the loss, so we can log the average loss to the user, allowing them to monitor the training.\n",
    "\n",
    "The array of losses accumulated during the epoch is returned as return value of the train epoch function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0cda7c5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class NeuMF(NeuMF):\n",
    "    def _compute_loss(\n",
    "        self, positive_scores: torch.FloatTensor, negative_scores: torch.FloatTensor\n",
    "    ) -> torch.FloatTensor:\n",
    "        \"\"\"Compute the Square Error loss given recommendations \n",
    "        for positive items, and sampled negatives.\n",
    "        \"\"\"\n",
    "\n",
    "        mse = nn.MSELoss(reduction=\"sum\")\n",
    "        return mse(positive_scores, torch.ones_like(positive_scores, dtype=torch.float)) + mse(\n",
    "            negative_scores, torch.zeros_like(negative_scores, dtype=torch.float)\n",
    "        )\n",
    "\n",
    "    def _construct_negative_prediction_input(self, users, negatives):\n",
    "        \"\"\"Construct the prediction input given a 1D user tensor and a 2D negatives tensor.\n",
    "        \n",
    "        Since negatives has shape |batch| x U, and users is a 1d vector,\n",
    "        these need to be turned into two 1D vectors of shape |batch| * U\n",
    "\n",
    "        First the users as a row are stacked U times and transposed,\n",
    "        so that this is also a batch x U tensor.\n",
    "        Then both are reshaped to remove the 2nd dimension, \n",
    "        resulting in a single long 1d vector.\n",
    "        \"\"\"\n",
    "        return (\n",
    "            users.repeat(self.n_negatives_per_positive, 1).T.reshape(-1), \n",
    "            negatives.reshape(-1)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52c35ae",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Squared Loss. This is simply the sum of the square of the error in the prediction. Because we are reconstructing a binary matrix, the target value for positives is 1 and 0 for the negatives.\n",
    "\n",
    "construction of input:\n",
    "repeat the users tensor such that the correct users are still associated with the correct negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b93dbaa3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class NeuMF(NeuMF):\n",
    "    def _batch_predict(self, X: csr_matrix, users: List[int]) -> csr_matrix:\n",
    "        \"\"\"Generate recommendations for each of the users.\"\"\"\n",
    "\n",
    "        X_pred = lil_matrix(X.shape)\n",
    "        if users is None:\n",
    "            users = get_users(X)\n",
    "\n",
    "        _, n_items = X.shape\n",
    "        n_users = len(users)\n",
    "\n",
    "        # Create two 1D arrays such that each item gets a score for each of the users.\n",
    "        # The user tensor contains the users in order (eg. [1, 1, 2, 2]), \n",
    "        # such that the items are the item indices repeated (eg. [0, 1, 2, 0, 1, 2]).\n",
    "        user_tensor = torch.LongTensor(users).repeat(n_items, 1).T.reshape(-1).to(self.device)\n",
    "        item_tensor = torch.arange(n_items).repeat(n_users).to(self.device)\n",
    "\n",
    "        X_pred[users] = self.model_(user_tensor, item_tensor).detach().cpu().numpy().reshape(n_users, n_items)\n",
    "        return X_pred.tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d1eeff",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "For any decent size dataset computing a prediction for each user at once requires too much RAM to be feasible. \n",
    "Instead we compute the recommendations for a batch of users, and then only keep topK recommendations.\n",
    "\n",
    "In the implementation our only task is to generate the correct tensors to give as input to the module\n",
    "We can then rely on the fitted model to generate the recommendation scores, the base class to make sure the model is in prediction mode as well as pruning recommendations and constructing the full prediction output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ca8f574",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "TIMESTAMP_IX = 'ts'\n",
    "ITEM_IX = 'iid'\n",
    "USER_IX = 'uid'\n",
    "\n",
    "data = {\n",
    "    TIMESTAMP_IX: [3, 2, 1, 4, 0, 1, 2, 4, 0, 1, 2],\n",
    "    ITEM_IX: [0, 1, 2, 3, 0, 1, 2, 4, 0, 1, 2],\n",
    "    USER_IX: [0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5],\n",
    "}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "mat = InteractionMatrix(df, ITEM_IX, USER_IX, timestamp_ix=TIMESTAMP_IX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1cf3329d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512 10 0.01 ndcg\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     params \u001b[38;5;241m=\u001b[39m [np \u001b[38;5;28;01mfor\u001b[39;00m np \u001b[38;5;129;01min\u001b[39;00m a\u001b[38;5;241m.\u001b[39mmodel_\u001b[38;5;241m.\u001b[39mnamed_parameters() \u001b[38;5;28;01mif\u001b[39;00m np[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mrequires_grad]\n\u001b[1;32m     18\u001b[0m     assert_changed(params_before, params, device)\n\u001b[0;32m---> 20\u001b[0m \u001b[43mtest_training_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_values\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36mtest_training_epoch\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     16\u001b[0m     a\u001b[38;5;241m.\u001b[39m_train_epoch(X)\n\u001b[1;32m     17\u001b[0m params \u001b[38;5;241m=\u001b[39m [np \u001b[38;5;28;01mfor\u001b[39;00m np \u001b[38;5;129;01min\u001b[39;00m a\u001b[38;5;241m.\u001b[39mmodel_\u001b[38;5;241m.\u001b[39mnamed_parameters() \u001b[38;5;28;01mif\u001b[39;00m np[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mrequires_grad]\n\u001b[0;32m---> 18\u001b[0m \u001b[43massert_changed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_before\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/recsys_demo/lib/python3.9/site-packages/recpack/tests/test_algorithms/util.py:8\u001b[0m, in \u001b[0;36massert_changed\u001b[0;34m(params_before, params_after, device)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21massert_changed\u001b[39m(params_before, params_after, device):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# check if variables have changed\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (_, p0), (_, p1) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(params_before, params_after):\n\u001b[0;32m----> 8\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mequal(p0\u001b[38;5;241m.\u001b[39mto(device), p1\u001b[38;5;241m.\u001b[39mto(device))\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from recpack.tests.test_algorithms.util import assert_changed, assert_same\n",
    "\n",
    "def test_training_epoch(X):\n",
    "    a = NeuMF(\n",
    "        predictive_factors=2, \n",
    "        n_negatives_per_positive=2,\n",
    "        exact_sampling=True\n",
    "    )\n",
    "    device = a.device\n",
    "    a._init_model(X)\n",
    "\n",
    "    # Each training epoch should update the parameters\n",
    "    params = [np for np in a.model_.named_parameters() if np[1].requires_grad]\n",
    "    params_before = [(name, p.clone()) for (name, p) in params]\n",
    "    for _ in range(5):\n",
    "        a._train_epoch(X)\n",
    "    params = [np for np in a.model_.named_parameters() if np[1].requires_grad]\n",
    "    assert_changed(params_before, params, device)\n",
    "\n",
    "test_training_epoch(mat.binary_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "814b0473",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512 10 0.01 ndcg\n",
      "2022-06-22 16:18:41,417 - base - recpack - INFO - Processed epoch 0 in 0.01 s.Batch Training Loss = 10.4516\n",
      "2022-06-22 16:18:41,423 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.6405398473870061, which is better than previous iterations.\n",
      "2022-06-22 16:18:41,424 - base - recpack - INFO - Model improved. Storing better model.\n",
      "2022-06-22 16:18:41,428 - base - recpack - INFO - Evaluation at end of 0 took 0.01 s.\n",
      "2022-06-22 16:18:41,439 - base - recpack - INFO - Processed epoch 1 in 0.01 s.Batch Training Loss = 10.3924\n",
      "2022-06-22 16:18:41,444 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.6028241164651866, which is worse than previous iterations.\n",
      "2022-06-22 16:18:41,445 - base - recpack - INFO - Evaluation at end of 1 took 0.01 s.\n",
      "2022-06-22 16:18:41,457 - base - recpack - INFO - Processed epoch 2 in 0.01 s.Batch Training Loss = 10.3358\n",
      "2022-06-22 16:18:41,462 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.6028241164651866, which is worse than previous iterations.\n",
      "2022-06-22 16:18:41,463 - base - recpack - INFO - Evaluation at end of 2 took 0.01 s.\n",
      "2022-06-22 16:18:41,480 - base - recpack - INFO - Processed epoch 3 in 0.02 s.Batch Training Loss = 10.2785\n",
      "2022-06-22 16:18:41,485 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.6028241164651866, which is worse than previous iterations.\n",
      "2022-06-22 16:18:41,486 - base - recpack - INFO - Evaluation at end of 3 took 0.00 s.\n",
      "2022-06-22 16:18:41,502 - base - recpack - INFO - Processed epoch 4 in 0.02 s.Batch Training Loss = 10.2206\n",
      "2022-06-22 16:18:41,507 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.6028241164651866, which is worse than previous iterations.\n",
      "2022-06-22 16:18:41,508 - base - recpack - INFO - Evaluation at end of 4 took 0.00 s.\n",
      "2022-06-22 16:18:41,521 - base - recpack - INFO - Processed epoch 5 in 0.01 s.Batch Training Loss = 10.1621\n",
      "2022-06-22 16:18:41,526 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.6028241164651866, which is worse than previous iterations.\n",
      "2022-06-22 16:18:41,526 - base - recpack - INFO - Evaluation at end of 5 took 0.00 s.\n",
      "2022-06-22 16:18:41,541 - base - recpack - INFO - Processed epoch 6 in 0.01 s.Batch Training Loss = 10.1032\n",
      "2022-06-22 16:18:41,547 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.6028241164651866, which is worse than previous iterations.\n",
      "2022-06-22 16:18:41,548 - base - recpack - INFO - Evaluation at end of 6 took 0.01 s.\n",
      "2022-06-22 16:18:41,563 - base - recpack - INFO - Processed epoch 7 in 0.01 s.Batch Training Loss = 10.0437\n",
      "2022-06-22 16:18:41,568 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.6028241164651866, which is worse than previous iterations.\n",
      "2022-06-22 16:18:41,569 - base - recpack - INFO - Evaluation at end of 7 took 0.01 s.\n",
      "2022-06-22 16:18:41,581 - base - recpack - INFO - Processed epoch 8 in 0.01 s.Batch Training Loss = 9.9837\n",
      "2022-06-22 16:18:41,586 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.6028241164651866, which is worse than previous iterations.\n",
      "2022-06-22 16:18:41,587 - base - recpack - INFO - Evaluation at end of 8 took 0.00 s.\n",
      "2022-06-22 16:18:41,597 - base - recpack - INFO - Processed epoch 9 in 0.01 s.Batch Training Loss = 9.9233\n",
      "2022-06-22 16:18:41,602 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.6028241164651866, which is worse than previous iterations.\n",
      "2022-06-22 16:18:41,603 - base - recpack - INFO - Evaluation at end of 9 took 0.01 s.\n",
      "2022-06-22 16:18:41,606 - base - recpack - INFO - Fitting NeuMF complete - Took 0.205s\n",
      "512 10 0.01 ndcg\n",
      "2022-06-22 16:18:41,623 - base - recpack - INFO - Processed epoch 0 in 0.01 s.Batch Training Loss = 9.9468\n",
      "2022-06-22 16:18:41,629 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.7737582122087544, which is better than previous iterations.\n",
      "2022-06-22 16:18:41,630 - base - recpack - INFO - Model improved. Storing better model.\n",
      "2022-06-22 16:18:41,634 - base - recpack - INFO - Evaluation at end of 0 took 0.01 s.\n",
      "2022-06-22 16:18:41,650 - base - recpack - INFO - Processed epoch 1 in 0.02 s.Batch Training Loss = 9.8602\n",
      "2022-06-22 16:18:41,655 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.7372530491956414, which is worse than previous iterations.\n",
      "2022-06-22 16:18:41,656 - base - recpack - INFO - Evaluation at end of 1 took 0.00 s.\n",
      "2022-06-22 16:18:41,672 - base - recpack - INFO - Processed epoch 2 in 0.02 s.Batch Training Loss = 9.8002\n",
      "2022-06-22 16:18:41,678 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.7301688035606183, which is worse than previous iterations.\n",
      "2022-06-22 16:18:41,678 - base - recpack - INFO - Evaluation at end of 2 took 0.01 s.\n",
      "2022-06-22 16:18:41,689 - base - recpack - INFO - Processed epoch 3 in 0.01 s.Batch Training Loss = 9.7273\n",
      "2022-06-22 16:18:41,695 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.7301688035606183, which is worse than previous iterations.\n",
      "2022-06-22 16:18:41,696 - base - recpack - INFO - Evaluation at end of 3 took 0.01 s.\n",
      "2022-06-22 16:18:41,710 - base - recpack - INFO - Processed epoch 4 in 0.01 s.Batch Training Loss = 9.6632\n",
      "2022-06-22 16:18:41,716 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.7443372948306645, which is worse than previous iterations.\n",
      "2022-06-22 16:18:41,717 - base - recpack - INFO - Evaluation at end of 4 took 0.01 s.\n",
      "2022-06-22 16:18:41,728 - base - recpack - INFO - Processed epoch 5 in 0.01 s.Batch Training Loss = 9.5924\n",
      "2022-06-22 16:18:41,733 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.7372530491956414, which is worse than previous iterations.\n",
      "2022-06-22 16:18:41,733 - base - recpack - INFO - Evaluation at end of 5 took 0.01 s.\n",
      "2022-06-22 16:18:41,745 - base - recpack - INFO - Processed epoch 6 in 0.01 s.Batch Training Loss = 9.5142\n",
      "2022-06-22 16:18:41,751 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.7372530491956414, which is worse than previous iterations.\n",
      "2022-06-22 16:18:41,751 - base - recpack - INFO - Evaluation at end of 6 took 0.01 s.\n",
      "2022-06-22 16:18:41,763 - base - recpack - INFO - Processed epoch 7 in 0.01 s.Batch Training Loss = 9.4398\n",
      "2022-06-22 16:18:41,768 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.7372530491956414, which is worse than previous iterations.\n",
      "2022-06-22 16:18:41,769 - base - recpack - INFO - Evaluation at end of 7 took 0.00 s.\n",
      "2022-06-22 16:18:41,788 - base - recpack - INFO - Processed epoch 8 in 0.02 s.Batch Training Loss = 9.3403\n",
      "2022-06-22 16:18:41,793 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.7372530491956414, which is worse than previous iterations.\n",
      "2022-06-22 16:18:41,794 - base - recpack - INFO - Evaluation at end of 8 took 0.01 s.\n",
      "2022-06-22 16:18:41,806 - base - recpack - INFO - Processed epoch 9 in 0.01 s.Batch Training Loss = 9.2433\n",
      "2022-06-22 16:18:41,811 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.7372530491956414, which is worse than previous iterations.\n",
      "2022-06-22 16:18:41,812 - base - recpack - INFO - Evaluation at end of 9 took 0.01 s.\n",
      "2022-06-22 16:18:41,816 - base - recpack - INFO - Fitting NeuMF complete - Took 0.205s\n",
      "512 10 0.01 ndcg\n",
      "2022-06-22 16:18:41,833 - base - recpack - INFO - Processed epoch 0 in 0.01 s.Batch Training Loss = 9.6960\n",
      "2022-06-22 16:18:41,839 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.7837146108282446, which is better than previous iterations.\n",
      "2022-06-22 16:18:41,839 - base - recpack - INFO - Model improved. Storing better model.\n",
      "2022-06-22 16:18:41,843 - base - recpack - INFO - Evaluation at end of 0 took 0.01 s.\n",
      "2022-06-22 16:18:41,854 - base - recpack - INFO - Processed epoch 1 in 0.01 s.Batch Training Loss = 9.5650\n",
      "2022-06-22 16:18:41,859 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.7863204548293853, which is better than previous iterations.\n",
      "2022-06-22 16:18:41,860 - base - recpack - INFO - Model improved. Storing better model.\n",
      "2022-06-22 16:18:41,863 - base - recpack - INFO - Evaluation at end of 1 took 0.01 s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-22 16:18:41,883 - base - recpack - INFO - Processed epoch 2 in 0.02 s.Batch Training Loss = 9.4404\n",
      "2022-06-22 16:18:41,888 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.7838062179781108, which is worse than previous iterations.\n",
      "2022-06-22 16:18:41,889 - base - recpack - INFO - Evaluation at end of 2 took 0.00 s.\n",
      "2022-06-22 16:18:41,898 - base - recpack - INFO - Processed epoch 3 in 0.01 s.Batch Training Loss = 9.3179\n",
      "2022-06-22 16:18:41,903 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.7659479478689263, which is worse than previous iterations.\n",
      "2022-06-22 16:18:41,904 - base - recpack - INFO - Evaluation at end of 3 took 0.00 s.\n",
      "2022-06-22 16:18:41,915 - base - recpack - INFO - Processed epoch 4 in 0.01 s.Batch Training Loss = 9.2018\n",
      "2022-06-22 16:18:41,920 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.8524021713512242, which is better than previous iterations.\n",
      "2022-06-22 16:18:41,920 - base - recpack - INFO - Model improved. Storing better model.\n",
      "2022-06-22 16:18:41,923 - base - recpack - INFO - Evaluation at end of 4 took 0.01 s.\n",
      "2022-06-22 16:18:41,943 - base - recpack - INFO - Processed epoch 5 in 0.02 s.Batch Training Loss = 9.0884\n",
      "2022-06-22 16:18:41,949 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.8319380572408991, which is worse than previous iterations.\n",
      "2022-06-22 16:18:41,949 - base - recpack - INFO - Evaluation at end of 5 took 0.01 s.\n",
      "2022-06-22 16:18:41,970 - base - recpack - INFO - Processed epoch 6 in 0.02 s.Batch Training Loss = 8.9781\n",
      "2022-06-22 16:18:41,975 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.6988666359953621, which is worse than previous iterations.\n",
      "2022-06-22 16:18:41,975 - base - recpack - INFO - Evaluation at end of 6 took 0.00 s.\n",
      "2022-06-22 16:18:41,988 - base - recpack - INFO - Processed epoch 7 in 0.01 s.Batch Training Loss = 8.8724\n",
      "2022-06-22 16:18:41,992 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.6610039614973312, which is worse than previous iterations.\n",
      "2022-06-22 16:18:41,993 - base - recpack - INFO - Evaluation at end of 7 took 0.00 s.\n",
      "2022-06-22 16:18:42,004 - base - recpack - INFO - Processed epoch 8 in 0.01 s.Batch Training Loss = 8.7675\n",
      "2022-06-22 16:18:42,008 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.6232882305755117, which is worse than previous iterations.\n",
      "2022-06-22 16:18:42,009 - base - recpack - INFO - Evaluation at end of 8 took 0.00 s.\n",
      "2022-06-22 16:18:42,022 - base - recpack - INFO - Processed epoch 9 in 0.01 s.Batch Training Loss = 8.6667\n",
      "2022-06-22 16:18:42,027 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.6246457420604296, which is worse than previous iterations.\n",
      "2022-06-22 16:18:42,028 - base - recpack - INFO - Evaluation at end of 9 took 0.00 s.\n",
      "2022-06-22 16:18:42,031 - base - recpack - INFO - Fitting NeuMF complete - Took 0.211s\n"
     ]
    }
   ],
   "source": [
    "def test_batch_predict(mat, users):\n",
    "    a = NeuMF(\n",
    "        predictive_factors=2, \n",
    "        n_negatives_per_positive=2,\n",
    "        exact_sampling=True\n",
    "    )\n",
    "    device = a.device\n",
    "    a.fit(mat, (mat, mat))\n",
    "    params = [np for np in a.model_.named_parameters() if np[1].requires_grad]\n",
    "    params_before = [(name, p.clone()) for (name, p) in params]\n",
    "\n",
    "    pred = a._batch_predict(mat.users_in(users), users=users)\n",
    "\n",
    "    assert pred.shape == mat.shape\n",
    "    np.testing.assert_array_equal(pred.sum(axis=1).nonzero()[0], users)\n",
    "\n",
    "    params = [np for np in a.model_.named_parameters() if np[1].requires_grad]\n",
    "    assert_same(params_before, params, device)\n",
    "\n",
    "    \n",
    "\n",
    "test_batch_predict(mat, [0, 1])\n",
    "test_batch_predict(mat, [0])\n",
    "test_batch_predict(mat, [0, 1, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed4d2e7d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512 10 0.01 ndcg\n",
      "512 10 0.01 ndcg\n"
     ]
    }
   ],
   "source": [
    "def test_negative_input_construction(users, negatives, U):\n",
    "    \n",
    "    a = NeuMF(\n",
    "        predictive_factors=8, \n",
    "        n_negatives_per_positive=U\n",
    "    )\n",
    "    \n",
    "    num_users = users.shape[0]\n",
    "    users_input, negatives_input = a._construct_negative_prediction_input(users, negatives)\n",
    "    assert users_input.shape == negatives_input.shape\n",
    "    assert len(users_input.shape) == 1 # 1d vectors\n",
    "    \n",
    "    # Check that both are in the right order (each user is repeated U times before the next user is present)\n",
    "    for ix in range(users_input.shape[0]):\n",
    "        assert users_input[ix] == users[ix // U]\n",
    "        assert negatives_input[ix] == negatives[ix // U, ix % U]\n",
    "\n",
    "test_negative_input_construction(torch.LongTensor([4, 5, 6]), torch.LongTensor([[1, 2], [1, 2], [1, 2]]), U=2)\n",
    "test_negative_input_construction(torch.LongTensor([4, 5, 6]), torch.LongTensor([[1], [1], [1]]), U=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f31c898",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 20 0.02 ndcg\n",
      "2022-06-22 16:18:42,629 - base - recpack - INFO - Processed epoch 0 in 0.03 s.Batch Training Loss = 0.5015\n",
      "2022-06-22 16:18:42,640 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.7633421038677857, which is better than previous iterations.\n",
      "2022-06-22 16:18:42,641 - base - recpack - INFO - Model improved. Storing better model.\n",
      "2022-06-22 16:18:42,645 - base - recpack - INFO - Evaluation at end of 0 took 0.02 s.\n",
      "2022-06-22 16:18:42,678 - base - recpack - INFO - Processed epoch 1 in 0.03 s.Batch Training Loss = 0.5001\n",
      "2022-06-22 16:18:42,690 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.7819889967717142, which is better than previous iterations.\n",
      "2022-06-22 16:18:42,691 - base - recpack - INFO - Model improved. Storing better model.\n",
      "2022-06-22 16:18:42,695 - base - recpack - INFO - Evaluation at end of 1 took 0.02 s.\n",
      "2022-06-22 16:18:42,726 - base - recpack - INFO - Processed epoch 2 in 0.03 s.Batch Training Loss = 0.4998\n",
      "2022-06-22 16:18:42,738 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.7908904636131339, which is better than previous iterations.\n",
      "2022-06-22 16:18:42,739 - base - recpack - INFO - Model improved. Storing better model.\n",
      "2022-06-22 16:18:42,742 - base - recpack - INFO - Evaluation at end of 2 took 0.01 s.\n",
      "2022-06-22 16:18:42,776 - base - recpack - INFO - Processed epoch 3 in 0.03 s.Batch Training Loss = 0.4977\n",
      "2022-06-22 16:18:42,787 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.7690688380178908, which is worse than previous iterations.\n",
      "2022-06-22 16:18:42,788 - base - recpack - INFO - Evaluation at end of 3 took 0.01 s.\n",
      "2022-06-22 16:18:42,816 - base - recpack - INFO - Processed epoch 4 in 0.03 s.Batch Training Loss = 0.4829\n",
      "2022-06-22 16:18:42,828 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.7690688380178909, which is worse than previous iterations.\n",
      "2022-06-22 16:18:42,828 - base - recpack - INFO - Evaluation at end of 4 took 0.01 s.\n",
      "2022-06-22 16:18:42,862 - base - recpack - INFO - Processed epoch 5 in 0.03 s.Batch Training Loss = 0.4504\n",
      "2022-06-22 16:18:42,872 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.7690688380178909, which is worse than previous iterations.\n",
      "2022-06-22 16:18:42,873 - base - recpack - INFO - Evaluation at end of 5 took 0.01 s.\n",
      "2022-06-22 16:18:42,906 - base - recpack - INFO - Processed epoch 6 in 0.03 s.Batch Training Loss = 0.4387\n",
      "2022-06-22 16:18:42,918 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.743126726921958, which is worse than previous iterations.\n",
      "2022-06-22 16:18:42,919 - base - recpack - INFO - Evaluation at end of 6 took 0.01 s.\n",
      "2022-06-22 16:18:42,954 - base - recpack - INFO - Processed epoch 7 in 0.03 s.Batch Training Loss = 0.3989\n",
      "2022-06-22 16:18:42,966 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.9545933701454672, which is better than previous iterations.\n",
      "2022-06-22 16:18:42,967 - base - recpack - INFO - Model improved. Storing better model.\n",
      "2022-06-22 16:18:42,970 - base - recpack - INFO - Evaluation at end of 7 took 0.02 s.\n",
      "2022-06-22 16:18:43,001 - base - recpack - INFO - Processed epoch 8 in 0.03 s.Batch Training Loss = 0.3235\n",
      "2022-06-22 16:18:43,013 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.9750574842557924, which is better than previous iterations.\n",
      "2022-06-22 16:18:43,014 - base - recpack - INFO - Model improved. Storing better model.\n",
      "2022-06-22 16:18:43,017 - base - recpack - INFO - Evaluation at end of 8 took 0.02 s.\n",
      "2022-06-22 16:18:43,051 - base - recpack - INFO - Processed epoch 9 in 0.03 s.Batch Training Loss = 0.3258\n",
      "2022-06-22 16:18:43,062 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.9999999999999999, which is better than previous iterations.\n",
      "2022-06-22 16:18:43,063 - base - recpack - INFO - Model improved. Storing better model.\n",
      "2022-06-22 16:18:43,066 - base - recpack - INFO - Evaluation at end of 9 took 0.01 s.\n",
      "2022-06-22 16:18:43,101 - base - recpack - INFO - Processed epoch 10 in 0.03 s.Batch Training Loss = 0.2526\n",
      "2022-06-22 16:18:43,112 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.9489044006028783, which is worse than previous iterations.\n",
      "2022-06-22 16:18:43,113 - base - recpack - INFO - Evaluation at end of 10 took 0.01 s.\n",
      "2022-06-22 16:18:43,143 - base - recpack - INFO - Processed epoch 11 in 0.03 s.Batch Training Loss = 0.1978\n",
      "2022-06-22 16:18:43,154 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.9355245321275762, which is worse than previous iterations.\n",
      "2022-06-22 16:18:43,155 - base - recpack - INFO - Evaluation at end of 11 took 0.01 s.\n",
      "2022-06-22 16:18:43,190 - base - recpack - INFO - Processed epoch 12 in 0.03 s.Batch Training Loss = 0.1661\n",
      "2022-06-22 16:18:43,203 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.9251084237866075, which is worse than previous iterations.\n",
      "2022-06-22 16:18:43,203 - base - recpack - INFO - Evaluation at end of 12 took 0.01 s.\n",
      "2022-06-22 16:18:43,234 - base - recpack - INFO - Processed epoch 13 in 0.03 s.Batch Training Loss = 0.2142\n",
      "2022-06-22 16:18:43,245 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.9866201315246979, which is worse than previous iterations.\n",
      "2022-06-22 16:18:43,246 - base - recpack - INFO - Evaluation at end of 13 took 0.01 s.\n",
      "2022-06-22 16:18:43,276 - base - recpack - INFO - Processed epoch 14 in 0.03 s.Batch Training Loss = 0.1242\n",
      "2022-06-22 16:18:43,287 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.9251084237866075, which is worse than previous iterations.\n",
      "2022-06-22 16:18:43,287 - base - recpack - INFO - Evaluation at end of 14 took 0.01 s.\n",
      "2022-06-22 16:18:43,320 - base - recpack - INFO - Processed epoch 15 in 0.03 s.Batch Training Loss = 0.0991\n",
      "2022-06-22 16:18:43,332 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.9866201315246979, which is worse than previous iterations.\n",
      "2022-06-22 16:18:43,332 - base - recpack - INFO - Evaluation at end of 15 took 0.01 s.\n",
      "2022-06-22 16:18:43,371 - base - recpack - INFO - Processed epoch 16 in 0.04 s.Batch Training Loss = 0.1188\n",
      "2022-06-22 16:18:43,384 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.9866201315246979, which is worse than previous iterations.\n",
      "2022-06-22 16:18:43,385 - base - recpack - INFO - Evaluation at end of 16 took 0.01 s.\n",
      "2022-06-22 16:18:43,417 - base - recpack - INFO - Processed epoch 17 in 0.03 s.Batch Training Loss = 0.1588\n",
      "2022-06-22 16:18:43,429 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.9489044006028784, which is worse than previous iterations.\n",
      "2022-06-22 16:18:43,429 - base - recpack - INFO - Evaluation at end of 17 took 0.01 s.\n",
      "2022-06-22 16:18:43,463 - base - recpack - INFO - Processed epoch 18 in 0.03 s.Batch Training Loss = 0.0280\n",
      "2022-06-22 16:18:43,476 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.9866201315246979, which is worse than previous iterations.\n",
      "2022-06-22 16:18:43,477 - base - recpack - INFO - Evaluation at end of 18 took 0.01 s.\n",
      "2022-06-22 16:18:43,507 - base - recpack - INFO - Processed epoch 19 in 0.03 s.Batch Training Loss = 0.0902\n",
      "2022-06-22 16:18:43,518 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.9866201315246979, which is worse than previous iterations.\n",
      "2022-06-22 16:18:43,519 - base - recpack - INFO - Evaluation at end of 19 took 0.01 s.\n",
      "2022-06-22 16:18:43,523 - base - recpack - INFO - Fitting NeuMF complete - Took 0.929s\n"
     ]
    }
   ],
   "source": [
    "def test_overfit(mat):\n",
    "    m = NeuMF(\n",
    "        predictive_factors=5,\n",
    "        batch_size=1,\n",
    "        max_epochs=20,\n",
    "        learning_rate=0.02,\n",
    "        stopping_criterion=\"ndcg\",\n",
    "        n_negatives_per_positive=1,\n",
    "    )\n",
    "\n",
    "    # set sampler to exact sampling\n",
    "    m.sampler.exact = True\n",
    "    m.fit(mat, (mat, mat))\n",
    "    bin_mat = mat.binary_values\n",
    "    pred = m.predict(mat.binary_values).toarray()\n",
    "    for user in mat.active_users:\n",
    "        # The model should have overfitted, so that the visited items have the highest similarities\n",
    "        positives = bin_mat[user].nonzero()[1]\n",
    "        negatives = list(set(range(mat.shape[1])) - set(positives))\n",
    "\n",
    "        for item in positives:\n",
    "            assert (pred[user][negatives] < pred[user, item]).all()\n",
    "            \n",
    "test_overfit(mat)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b414610",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Experiment\n",
    "\n",
    "Use RecPack Pipeline to compare the newly implemented algorithm to frequently used baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839a158a",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Now that we have implemented a new algorithm, we want to compare its performance to some baseline algorithms.\n",
    "This acts both as a sanity check in terms of performance, as an important step to make a contribution to the field.\n",
    "\n",
    "For easy experimentation RecPack provides a pipeline setup.\n",
    "\n",
    "We’ll use the PipelineBuilder to construct our pipeline, and once constructed run it to finally get comparative results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3225456",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from recpack.pipelines import PipelineBuilder\n",
    "from recpack.datasets import MovieLens25M\n",
    "from recpack.scenarios import WeakGeneralization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69260279",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We need the following components:\n",
    "\n",
    "- PipelineBuilder\n",
    "- A Dataset\n",
    "- A Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "345019f0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e8ccddf893044358dd406ca70518b55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12415224 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bab339895d546929207a3e7803ecef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12415224 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = MovieLens25M(\n",
    "#     path='/home/robinverachtert/datasets',\n",
    "    path='/Users/robinverachtert/workspace/doctorate/datasets/'\n",
    ")\n",
    "data = dataset.load_interaction_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47169d75",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Subsample to 1000 users to make it faster\n",
    "import numpy as np\n",
    "\n",
    "users = np.random.choice(list(data.active_users), 1000)\n",
    "data = data.users_in(users)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0452e6d",
   "metadata": {},
   "source": [
    "### Choosing the right scenario\n",
    "\n",
    "Add screenshot / list of available scenarios + Explain which we can't use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a5a8b9ab",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4e525195dc8476a9652193937fb6335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa61f0d994ac480cbd635637c19600a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scenario = WeakGeneralization(frac_data_in=0.8, validation=True)\n",
    "scenario.split(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5bdacd23",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2993874910.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [32]\u001b[0;36m\u001b[0m\n\u001b[0;31m    TODO: Add more detail on scenarios!\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba6aa1bc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from recpack.pipelines import ALGORITHM_REGISTRY\n",
    "ALGORITHM_REGISTRY.register('NeuMF', NeuMF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87b0e1d",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In order to use our new algorithm we need to register it with the algorithm registry.\n",
    "This will allow the pipeline to find the right class to use when constructing the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c46caa2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "builder = PipelineBuilder()\n",
    "builder.set_data_from_scenario(scenario)\n",
    "builder.set_optimisation_metric('NDCGK', K=10)\n",
    "builder.add_metric('NDCGK', K=10)\n",
    "builder.add_metric('CoverageK', K=10)\n",
    "\n",
    "builder.add_algorithm(\n",
    "    algorithm = 'NeuMF', \n",
    "    params = {\n",
    "        'batch_size': 128,\n",
    "        'max_epochs': 10,\n",
    "        'learning_rate': 0.01,\n",
    "        'stopping_criterion': 'ndcg',\n",
    "        'predict_topK': 20,\n",
    "        'n_negatives_per_positive': 3,\n",
    "        'dropout': 0.01\n",
    "    },\n",
    "    grid = {\n",
    "        'predictive_factors': [8, 16, 32],\n",
    "    }\n",
    ")\n",
    "\n",
    "builder.add_algorithm('Popularity', params={'K': 20})\n",
    "builder.add_algorithm('ItemKNN', grid={'similarity': ['conditional_probability', 'cosine']})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c5f30e",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Now we can construct our pipeline using the pipelineBuilder.\n",
    "\n",
    "We set the data from the scenario, select an optimisation metric, add metrics to evaluate on.\n",
    "And finally we add our algorithms and the parameters we want to use.\n",
    "If parameters should be optimised like the predictive factors here, we use grid search \n",
    "(more advanced methods are under development) \n",
    "\n",
    "We similarly add our baselines, and their parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "05c0a518",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = builder.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e0fd2e",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "When you call build(), the builder will perform some checks on the values you have set, and raise an error if anything is wrong. Eg. if you are expecting parameters to be optimised, you should have validation data as well as an optimisation metric. Validation data is part of the splitting process, while the optimisation metric should be set on the builder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3314c304",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47fe50b8730a47699141c12c9b5e396f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 10 0.01 ndcg\n",
      "2022-06-22 16:19:56,011 - base - recpack - INFO - Processed epoch 0 in 5.28 s.Batch Training Loss = 53.6188\n",
      "2022-06-22 16:20:11,113 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.07013975762846296, which is better than previous iterations.\n",
      "2022-06-22 16:20:11,114 - base - recpack - INFO - Model improved. Storing better model.\n",
      "2022-06-22 16:20:11,134 - base - recpack - INFO - Evaluation at end of 0 took 15.12 s.\n",
      "2022-06-22 16:20:16,309 - base - recpack - INFO - Processed epoch 1 in 5.17 s.Batch Training Loss = 42.9730\n",
      "2022-06-22 16:20:31,098 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.06826195413245605, which is worse than previous iterations.\n",
      "2022-06-22 16:20:31,099 - base - recpack - INFO - Evaluation at end of 1 took 14.79 s.\n",
      "2022-06-22 16:20:36,353 - base - recpack - INFO - Processed epoch 2 in 5.25 s.Batch Training Loss = 40.6114\n",
      "2022-06-22 16:20:51,654 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.0720450539478035, which is better than previous iterations.\n",
      "2022-06-22 16:20:51,655 - base - recpack - INFO - Model improved. Storing better model.\n",
      "2022-06-22 16:20:51,674 - base - recpack - INFO - Evaluation at end of 2 took 15.32 s.\n",
      "2022-06-22 16:20:57,114 - base - recpack - INFO - Processed epoch 3 in 5.44 s.Batch Training Loss = 39.7043\n",
      "2022-06-22 16:21:12,519 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.06495607132800234, which is worse than previous iterations.\n",
      "2022-06-22 16:21:12,520 - base - recpack - INFO - Evaluation at end of 3 took 15.41 s.\n",
      "2022-06-22 16:21:17,849 - base - recpack - INFO - Processed epoch 4 in 5.33 s.Batch Training Loss = 39.0028\n",
      "2022-06-22 16:21:32,860 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.0710938986243025, which is worse than previous iterations.\n",
      "2022-06-22 16:21:32,861 - base - recpack - INFO - Evaluation at end of 4 took 15.01 s.\n",
      "2022-06-22 16:21:38,099 - base - recpack - INFO - Processed epoch 5 in 5.24 s.Batch Training Loss = 38.4005\n",
      "2022-06-22 16:21:53,539 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.07597804973314203, which is better than previous iterations.\n",
      "2022-06-22 16:21:53,539 - base - recpack - INFO - Model improved. Storing better model.\n",
      "2022-06-22 16:21:53,557 - base - recpack - INFO - Evaluation at end of 5 took 15.46 s.\n",
      "2022-06-22 16:21:58,939 - base - recpack - INFO - Processed epoch 6 in 5.38 s.Batch Training Loss = 37.5675\n",
      "2022-06-22 16:22:14,663 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.07980072349464426, which is better than previous iterations.\n",
      "2022-06-22 16:22:14,664 - base - recpack - INFO - Model improved. Storing better model.\n",
      "2022-06-22 16:22:14,681 - base - recpack - INFO - Evaluation at end of 6 took 15.74 s.\n",
      "2022-06-22 16:22:20,064 - base - recpack - INFO - Processed epoch 7 in 5.38 s.Batch Training Loss = 36.5902\n",
      "2022-06-22 16:22:35,655 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.07609348925293805, which is worse than previous iterations.\n",
      "2022-06-22 16:22:35,655 - base - recpack - INFO - Evaluation at end of 7 took 15.59 s.\n",
      "2022-06-22 16:22:41,099 - base - recpack - INFO - Processed epoch 8 in 5.44 s.Batch Training Loss = 35.2336\n",
      "2022-06-22 16:22:56,546 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.0756260688193669, which is worse than previous iterations.\n",
      "2022-06-22 16:22:56,546 - base - recpack - INFO - Evaluation at end of 8 took 15.45 s.\n",
      "2022-06-22 16:23:01,774 - base - recpack - INFO - Processed epoch 9 in 5.23 s.Batch Training Loss = 33.9317\n",
      "2022-06-22 16:23:17,573 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.0781477653225301, which is worse than previous iterations.\n",
      "2022-06-22 16:23:17,573 - base - recpack - INFO - Evaluation at end of 9 took 15.80 s.\n",
      "2022-06-22 16:23:17,583 - base - recpack - INFO - Fitting NeuMF complete - Took 2.07e+02s\n",
      "128 10 0.01 ndcg\n",
      "2022-06-22 16:23:46,838 - base - recpack - INFO - Processed epoch 0 in 13.66 s.Batch Training Loss = 52.9700\n",
      "2022-06-22 16:24:08,781 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.05836976313094121, which is better than previous iterations.\n",
      "2022-06-22 16:24:08,782 - base - recpack - INFO - Model improved. Storing better model.\n",
      "2022-06-22 16:24:08,811 - base - recpack - INFO - Evaluation at end of 0 took 21.97 s.\n",
      "2022-06-22 16:24:21,727 - base - recpack - INFO - Processed epoch 1 in 12.92 s.Batch Training Loss = 41.9320\n",
      "2022-06-22 16:24:43,458 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.06985231596608127, which is better than previous iterations.\n",
      "2022-06-22 16:24:43,459 - base - recpack - INFO - Model improved. Storing better model.\n",
      "2022-06-22 16:24:43,499 - base - recpack - INFO - Evaluation at end of 1 took 21.77 s.\n",
      "2022-06-22 16:24:56,315 - base - recpack - INFO - Processed epoch 2 in 12.81 s.Batch Training Loss = 40.1135\n",
      "2022-06-22 16:25:17,438 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.06586508207381195, which is worse than previous iterations.\n",
      "2022-06-22 16:25:17,439 - base - recpack - INFO - Evaluation at end of 2 took 21.12 s.\n",
      "2022-06-22 16:25:30,366 - base - recpack - INFO - Processed epoch 3 in 12.93 s.Batch Training Loss = 38.8662\n",
      "2022-06-22 16:25:52,404 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.06922760742964788, which is worse than previous iterations.\n",
      "2022-06-22 16:25:52,405 - base - recpack - INFO - Evaluation at end of 3 took 22.04 s.\n",
      "2022-06-22 16:26:05,404 - base - recpack - INFO - Processed epoch 4 in 13.00 s.Batch Training Loss = 37.7014\n",
      "2022-06-22 16:26:26,754 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.06330270963832815, which is worse than previous iterations.\n",
      "2022-06-22 16:26:26,755 - base - recpack - INFO - Evaluation at end of 4 took 21.35 s.\n",
      "2022-06-22 16:26:39,542 - base - recpack - INFO - Processed epoch 5 in 12.79 s.Batch Training Loss = 35.6206\n",
      "2022-06-22 16:27:01,401 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.06939276269328576, which is worse than previous iterations.\n",
      "2022-06-22 16:27:01,402 - base - recpack - INFO - Evaluation at end of 5 took 21.86 s.\n",
      "2022-06-22 16:27:14,387 - base - recpack - INFO - Processed epoch 6 in 12.98 s.Batch Training Loss = 33.4491\n",
      "2022-06-22 16:27:36,753 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.07281620673956257, which is better than previous iterations.\n",
      "2022-06-22 16:27:36,754 - base - recpack - INFO - Model improved. Storing better model.\n",
      "2022-06-22 16:27:36,784 - base - recpack - INFO - Evaluation at end of 6 took 22.40 s.\n",
      "2022-06-22 16:27:50,379 - base - recpack - INFO - Processed epoch 7 in 13.59 s.Batch Training Loss = 31.1214\n",
      "2022-06-22 16:28:11,634 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.08266925090274452, which is better than previous iterations.\n",
      "2022-06-22 16:28:11,635 - base - recpack - INFO - Model improved. Storing better model.\n",
      "2022-06-22 16:28:11,666 - base - recpack - INFO - Evaluation at end of 7 took 21.29 s.\n",
      "2022-06-22 16:28:24,686 - base - recpack - INFO - Processed epoch 8 in 13.02 s.Batch Training Loss = 29.4384\n",
      "2022-06-22 16:28:46,791 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.0861429015243315, which is better than previous iterations.\n",
      "2022-06-22 16:28:46,792 - base - recpack - INFO - Model improved. Storing better model.\n",
      "2022-06-22 16:28:46,822 - base - recpack - INFO - Evaluation at end of 8 took 22.13 s.\n",
      "2022-06-22 16:29:00,040 - base - recpack - INFO - Processed epoch 9 in 13.22 s.Batch Training Loss = 28.1845\n",
      "2022-06-22 16:29:22,879 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.08246092810661176, which is worse than previous iterations.\n",
      "2022-06-22 16:29:22,880 - base - recpack - INFO - Evaluation at end of 9 took 22.84 s.\n",
      "2022-06-22 16:29:22,903 - base - recpack - INFO - Fitting NeuMF complete - Took 3.5e+02s\n",
      "128 10 0.01 ndcg\n",
      "2022-06-22 16:30:23,182 - base - recpack - INFO - Processed epoch 0 in 37.41 s.Batch Training Loss = 52.6171\n",
      "2022-06-22 16:31:02,245 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.06759082198004446, which is better than previous iterations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-22 16:31:02,245 - base - recpack - INFO - Model improved. Storing better model.\n",
      "2022-06-22 16:31:02,343 - base - recpack - INFO - Evaluation at end of 0 took 39.16 s.\n",
      "2022-06-22 16:31:37,609 - base - recpack - INFO - Processed epoch 1 in 35.27 s.Batch Training Loss = 42.2653\n",
      "2022-06-22 16:32:14,799 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.0668488138004705, which is worse than previous iterations.\n",
      "2022-06-22 16:32:14,800 - base - recpack - INFO - Evaluation at end of 1 took 37.19 s.\n",
      "2022-06-22 16:32:50,254 - base - recpack - INFO - Processed epoch 2 in 35.45 s.Batch Training Loss = 40.5017\n",
      "2022-06-22 16:33:27,841 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.06931486242399064, which is better than previous iterations.\n",
      "2022-06-22 16:33:27,842 - base - recpack - INFO - Model improved. Storing better model.\n",
      "2022-06-22 16:33:27,915 - base - recpack - INFO - Evaluation at end of 2 took 37.66 s.\n",
      "2022-06-22 16:34:03,518 - base - recpack - INFO - Processed epoch 3 in 35.60 s.Batch Training Loss = 39.4395\n",
      "2022-06-22 16:34:41,385 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.07003030146389151, which is better than previous iterations.\n",
      "2022-06-22 16:34:41,386 - base - recpack - INFO - Model improved. Storing better model.\n",
      "2022-06-22 16:34:41,442 - base - recpack - INFO - Evaluation at end of 3 took 37.92 s.\n",
      "2022-06-22 16:35:17,325 - base - recpack - INFO - Processed epoch 4 in 35.88 s.Batch Training Loss = 37.9144\n",
      "2022-06-22 16:35:55,290 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.07706687387045787, which is better than previous iterations.\n",
      "2022-06-22 16:35:55,291 - base - recpack - INFO - Model improved. Storing better model.\n",
      "2022-06-22 16:35:55,349 - base - recpack - INFO - Evaluation at end of 4 took 38.02 s.\n",
      "2022-06-22 16:36:31,094 - base - recpack - INFO - Processed epoch 5 in 35.74 s.Batch Training Loss = 35.4141\n",
      "2022-06-22 16:37:09,275 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.07017780103437748, which is worse than previous iterations.\n",
      "2022-06-22 16:37:09,276 - base - recpack - INFO - Evaluation at end of 5 took 38.18 s.\n",
      "2022-06-22 16:37:45,367 - base - recpack - INFO - Processed epoch 6 in 36.09 s.Batch Training Loss = 33.6103\n",
      "2022-06-22 16:38:22,725 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.07326441814906466, which is worse than previous iterations.\n",
      "2022-06-22 16:38:22,725 - base - recpack - INFO - Evaluation at end of 6 took 37.36 s.\n",
      "2022-06-22 16:38:58,735 - base - recpack - INFO - Processed epoch 7 in 36.01 s.Batch Training Loss = 30.8048\n",
      "2022-06-22 16:39:36,138 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.0811285954380648, which is better than previous iterations.\n",
      "2022-06-22 16:39:36,138 - base - recpack - INFO - Model improved. Storing better model.\n",
      "2022-06-22 16:39:36,206 - base - recpack - INFO - Evaluation at end of 7 took 37.47 s.\n",
      "2022-06-22 16:40:12,082 - base - recpack - INFO - Processed epoch 8 in 35.88 s.Batch Training Loss = 29.7392\n",
      "2022-06-22 16:40:50,614 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.0820054384476765, which is better than previous iterations.\n",
      "2022-06-22 16:40:50,614 - base - recpack - INFO - Model improved. Storing better model.\n",
      "2022-06-22 16:40:50,684 - base - recpack - INFO - Evaluation at end of 8 took 38.60 s.\n",
      "2022-06-22 16:41:26,769 - base - recpack - INFO - Processed epoch 9 in 36.08 s.Batch Training Loss = 28.0781\n",
      "2022-06-22 16:42:09,859 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.08494668940074873, which is better than previous iterations.\n",
      "2022-06-22 16:42:09,861 - base - recpack - INFO - Model improved. Storing better model.\n",
      "2022-06-22 16:42:09,964 - base - recpack - INFO - Evaluation at end of 9 took 43.19 s.\n",
      "2022-06-22 16:42:09,978 - base - recpack - INFO - Fitting NeuMF complete - Took 7.44e+02s\n",
      "128 10 0.01 ndcg\n",
      "2022-06-22 16:45:14,580 - base - recpack - INFO - Processed epoch 0 in 14.96 s.Batch Training Loss = 53.1782\n",
      "2022-06-22 16:45:37,375 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.062009303877084004, which is better than previous iterations.\n",
      "2022-06-22 16:45:37,376 - base - recpack - INFO - Model improved. Storing better model.\n",
      "2022-06-22 16:45:37,411 - base - recpack - INFO - Evaluation at end of 0 took 22.83 s.\n",
      "2022-06-22 16:45:53,690 - base - recpack - INFO - Processed epoch 1 in 16.28 s.Batch Training Loss = 42.1259\n",
      "2022-06-22 16:46:15,832 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.0726138557042641, which is better than previous iterations.\n",
      "2022-06-22 16:46:15,833 - base - recpack - INFO - Model improved. Storing better model.\n",
      "2022-06-22 16:46:15,869 - base - recpack - INFO - Evaluation at end of 1 took 22.18 s.\n",
      "2022-06-22 16:46:28,605 - base - recpack - INFO - Processed epoch 2 in 12.74 s.Batch Training Loss = 40.1789\n",
      "2022-06-22 16:46:50,319 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.07349707384291151, which is better than previous iterations.\n",
      "2022-06-22 16:46:50,319 - base - recpack - INFO - Model improved. Storing better model.\n",
      "2022-06-22 16:46:50,349 - base - recpack - INFO - Evaluation at end of 2 took 21.74 s.\n",
      "2022-06-22 16:47:02,919 - base - recpack - INFO - Processed epoch 3 in 12.57 s.Batch Training Loss = 39.2904\n",
      "2022-06-22 16:47:23,933 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.06831499145683123, which is worse than previous iterations.\n",
      "2022-06-22 16:47:23,934 - base - recpack - INFO - Evaluation at end of 3 took 21.01 s.\n",
      "2022-06-22 16:47:36,659 - base - recpack - INFO - Processed epoch 4 in 12.72 s.Batch Training Loss = 37.8796\n",
      "2022-06-22 16:47:58,763 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.06737218795051567, which is worse than previous iterations.\n",
      "2022-06-22 16:47:58,764 - base - recpack - INFO - Evaluation at end of 4 took 22.10 s.\n",
      "2022-06-22 16:48:12,262 - base - recpack - INFO - Processed epoch 5 in 13.50 s.Batch Training Loss = 37.3208\n",
      "2022-06-22 16:48:33,805 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.06953236950442285, which is worse than previous iterations.\n",
      "2022-06-22 16:48:33,806 - base - recpack - INFO - Evaluation at end of 5 took 21.54 s.\n",
      "2022-06-22 16:48:47,275 - base - recpack - INFO - Processed epoch 6 in 13.47 s.Batch Training Loss = 35.8344\n",
      "2022-06-22 16:49:09,092 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.06855275261919547, which is worse than previous iterations.\n",
      "2022-06-22 16:49:09,092 - base - recpack - INFO - Evaluation at end of 6 took 21.82 s.\n",
      "2022-06-22 16:49:21,836 - base - recpack - INFO - Processed epoch 7 in 12.74 s.Batch Training Loss = 34.4211\n",
      "2022-06-22 16:49:43,130 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.07785695879431467, which is better than previous iterations.\n",
      "2022-06-22 16:49:43,131 - base - recpack - INFO - Model improved. Storing better model.\n",
      "2022-06-22 16:49:43,162 - base - recpack - INFO - Evaluation at end of 7 took 21.32 s.\n",
      "2022-06-22 16:49:56,235 - base - recpack - INFO - Processed epoch 8 in 13.07 s.Batch Training Loss = 32.6610\n",
      "2022-06-22 16:50:17,970 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.0716174705344483, which is worse than previous iterations.\n",
      "2022-06-22 16:50:17,971 - base - recpack - INFO - Evaluation at end of 8 took 21.73 s.\n",
      "2022-06-22 16:50:31,596 - base - recpack - INFO - Processed epoch 9 in 13.62 s.Batch Training Loss = 31.0353\n",
      "2022-06-22 16:50:53,310 - stopping_criterion - recpack - INFO - StoppingCriterion has value 0.08085636411214076, which is better than previous iterations.\n",
      "2022-06-22 16:50:53,310 - base - recpack - INFO - Model improved. Storing better model.\n",
      "2022-06-22 16:50:53,341 - base - recpack - INFO - Evaluation at end of 9 took 21.74 s.\n",
      "2022-06-22 16:50:53,350 - base - recpack - INFO - Fitting NeuMF complete - Took 3.54e+02s\n",
      "2022-06-22 16:51:15,269 - base - recpack - INFO - Fitting Popularity complete - Took 0.031s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robinverachtert/.virtualenvs/recsys_demo/lib/python3.9/site-packages/scipy/sparse/_index.py:146: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-22 16:51:17,630 - base - recpack - INFO - Fitting ItemKNN complete - Took 2.28s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robinverachtert/.virtualenvs/recsys_demo/lib/python3.9/site-packages/recpack/algorithms/base.py:274: UserWarning: ItemKNN missing similar items for 12619 items.\n",
      "  warnings.warn(f\"{self.name} missing similar items for {missing} items.\")\n",
      "/Users/robinverachtert/.virtualenvs/recsys_demo/lib/python3.9/site-packages/scipy/sparse/_index.py:146: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-22 16:51:19,989 - base - recpack - INFO - Fitting ItemKNN complete - Took 2.04s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robinverachtert/.virtualenvs/recsys_demo/lib/python3.9/site-packages/recpack/algorithms/base.py:274: UserWarning: ItemKNN missing similar items for 12619 items.\n",
      "  warnings.warn(f\"{self.name} missing similar items for {missing} items.\")\n",
      "/Users/robinverachtert/.virtualenvs/recsys_demo/lib/python3.9/site-packages/scipy/sparse/_index.py:146: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-22 16:51:22,480 - base - recpack - INFO - Fitting ItemKNN complete - Took 2.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robinverachtert/.virtualenvs/recsys_demo/lib/python3.9/site-packages/recpack/algorithms/base.py:274: UserWarning: ItemKNN missing similar items for 12619 items.\n",
      "  warnings.warn(f\"{self.name} missing similar items for {missing} items.\")\n"
     ]
    }
   ],
   "source": [
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd49b9a6",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Running the pipeline goes through the algorithms in order, if there are parameters to optimise, those are optimised by training and evaluating(on the validation dataset) with each of the grid combinations, and the best one  is chosen according to the optimisation metric.\n",
    "\n",
    "Then a final training and evaluation (on the test dataset) with the selected metrics gives us the results which we can fetch once done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8af63fb1",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3a36b1d0",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>normalizeddiscountedcumulativegaink_10</th>\n",
       "      <th>coveragek_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NeuMF(U=3,batch_size=128,dropout=0.01,exact_sampling=False,keep_last=False,learning_rate=0.01,max_epochs=10,max_iter_no_change=5,min_improvement=0.0,num_components=16,predict_topK=20,save_best_to_file=False,seed=2404017312,stop_early=False,stopping_criterion=&lt;recpack.algorithms.stopping_criterion.StoppingCriterion object at 0x7fa205cd3c40&gt;)</th>\n",
       "      <td>0.045246</td>\n",
       "      <td>0.038300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Popularity(K=20)</th>\n",
       "      <td>0.094183</td>\n",
       "      <td>0.000313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ItemKNN(K=200,normalize=False,normalize_X=False,normalize_sim=False,pop_discount=None,similarity=cosine)</th>\n",
       "      <td>0.153369</td>\n",
       "      <td>0.083840</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    normalizeddiscountedcumulativegaink_10  coveragek_10\n",
       "NeuMF(U=3,batch_size=128,dropout=0.01,exact_sam...                                0.045246      0.038300\n",
       "Popularity(K=20)                                                                  0.094183      0.000313\n",
       "ItemKNN(K=200,normalize=False,normalize_X=False...                                0.153369      0.083840"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(pipeline.get_metrics()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "720b3479",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ndcgk_10</th>\n",
       "      <th>coveragek_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NeuMF</th>\n",
       "      <td>0.077922</td>\n",
       "      <td>0.004364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Popularity</th>\n",
       "      <td>0.079727</td>\n",
       "      <td>0.000502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ItemKNN</th>\n",
       "      <td>0.090906</td>\n",
       "      <td>0.107338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ndcgk_10  coveragek_10\n",
       "NeuMF       0.077922      0.004364\n",
       "Popularity  0.079727      0.000502\n",
       "ItemKNN     0.090906      0.107338"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_metrics_dataframe(short=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23742a86",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We see that the new algorithm does not perform as well as either ItemKNN or Popularity. This can be due to poor choices in hyper parameters, since tuning them is an important step"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
