import logging
import numpy as np

from scipy.sparse import csr_matrix, lil_matrix
from sklearn.utils.validation import check_is_fitted

from recpack.util import get_top_K_values
from recpack.algorithms import Algorithm
from recpack.data.matrix import Matrix, to_csr_matrix
from recpack.algorithms.util import (
    get_users,
    invert_np_array,
    union_csr_matrices,
    invert,
)

logger = logging.getLogger("recpack")


class KUNN(Algorithm):
    """
    KUNN Algorithm by Koen Verstrepen and Bart Goethals
    as described in paper 'Unifying Nearest Neighbors Collaborative Filtering' (10.1145/2645710.2645731)
    """

    def __init__(self, Ku: int = 100, Ki: int = 100):
        """
        Initialize the KUNN Algorithm but setting the number of K (top-K) for users and items.
        :param Ku: The top-K users which should be fit and predict for.
        :param Ki: The top-K items which should be fit and predict for.
        """
        super().__init__()
        self.Ku = Ku
        self.Ki = Ki

    def fit(self, X: Matrix) -> Algorithm:
        """
        Calculate the score matrix for items based on the binary matrix .
        :param X: Sparse binary user-item matrix which will be used to fit the algorithm.
        :return: The fitted KUNN Algorithm itself.
        """
        # To make sure the input is interpreted as a binary matrix
        X = to_csr_matrix(X, binary=True)

        self.training_interactions_ = csr_matrix(X, copy=True)  # Memoize X
        self.knn_i_ = self._fit_item_knn(X)

        return self

    def predict(self, X: Matrix) -> csr_matrix:
        """
        The prediction can easily be calculated by computing score matrix for users and add this to the already
        calculated score matrix for items.
        :param X: Sparse binary user-item matrix which will be used to do the predictions. The scores will returned for
        the users of this input matrix.
        :return: User-item matrix with the prediction scores as values.
        """
        check_is_fitted(self)

        X = to_csr_matrix(X, binary=True)

        knn_u = self._fit_user_knn(X)

        users_to_predict = get_users(X)

        # Combine the memoized training interactions with the predict interactions
        # We will only use this combination for the user we are trying to predict for!
        combined_interactions = union_csr_matrices(self.training_interactions_, X)

        # Compute user similarity,
        # Formula 10 in paper.
        # we'll pull the 1/ sqrt(c(u)c(i))
        # into the computation

        # Note that user_knn scores already compute the
        # inner sum in the user similarity formula
        # if we also include the division by sqrt(u)
        #
        # So we can compute user similarity as
        # KNN(u) @ (training_interactions / sqrt(count(i)))
        # Which will count occurrences of the target item,
        # weighted by the user similarity
        # and 1 / sqrt(count(i))
        item_counts = self.training_interactions_.sum(axis=0)

        user_similarity = csr_matrix(
            knn_u @ self.training_interactions_.multiply(invert(np.sqrt(item_counts)))
        )

        # Compute item similarities
        # Similar trick, 1/sqrt(c(i)) is already included in the item KNN computation.
        # Which computes then computes the inner sum.
        # The score is then generated by multiplying the combined matrix with the
        # Item KNN scores
        # And dividing by sqrt(c(u)), the square root of the user's interactions.
        user_counts = combined_interactions.sum(axis=1)
        item_similarity = csr_matrix(
            combined_interactions.multiply(invert(np.sqrt(user_counts))) @ self.knn_i_
        )

        similarity = item_similarity + user_similarity

        # We only need to set the similarities for users which we need to predict for
        # TODO: There is probably a way to optimise computation by not needing to
        # compute the similarities, similar to computation of user_knn.

        scores = lil_matrix(X.shape)
        scores[users_to_predict] = similarity[users_to_predict]

        scores = scores.tocsr()
        self._check_prediction(scores, X)

        return scores

    def _fit_item_knn(self, X: csr_matrix) -> csr_matrix:
        """
        Helper method to compute the Item KNN, used in the KUNN implementation.
        @param X: Sparse binary user-item matrix
        @return: Item KNN matrix for X
        """

        user_counts = X.sum(axis=1)
        item_counts = X.sum(axis=0)

        item_to_item_similarity = X.multiply(invert(np.sqrt(user_counts))).multiply(
            invert(np.sqrt(item_counts))
        ).T @ X.multiply(invert(np.sqrt(item_counts)))

        # Eliminate self-similarity
        item_to_item_similarity.setdiag(0)

        return get_top_K_values(item_to_item_similarity, self.Ki).T

    def _fit_user_knn(self, X: csr_matrix) -> csr_matrix:
        """
        Helper method to compute the User KNN, used in the KUNN implementation. The memoized training interactions are
        used to compute the user similarities.
        @param X: Sparse binary user-item matrix
        @return: User KNN matrix for X
        """
        users_to_predict = get_users(X)

        # Combine the memoized training interactions with the predict interactions
        combined_interactions = union_csr_matrices(self.training_interactions_, X)

        # Cut combined interactions to only nonzero users in prediction matrix.
        mask = np.zeros(combined_interactions.shape[0])
        mask[users_to_predict] = 1
        # Turn mask into a column vector
        mask = mask.reshape(mask.shape[0], 1)
        # Select the interactions for nonzero users in mask
        print(combined_interactions.shape, mask.shape)
        combined_interactions_selected_users = csr_matrix(
            combined_interactions.multiply(mask)
        )

        # Compute the interactions that are only in the prediction matrix.
        combined_interactions_only_predict = (
            combined_interactions_selected_users
            - self.training_interactions_.multiply(mask)
        )

        # Count the number of interactions per user for which we need to predict
        # This count is based on the union of train and predict data
        pred_user_interaction_counts = combined_interactions_selected_users.sum(axis=1)

        # Counts based on only training data
        train_user_counts = self.training_interactions_.sum(axis=1)
        train_item_counts = self.training_interactions_.sum(axis=0)

        # Compute the c(i) values in the paper
        # Because we have to account for items that occur both in train and predict,
        # but can only use interactions in the X matrix for the user we are computing
        # similarities for (avoid leakage of data),
        # we need to add 1 to the training counts in some occasions.
        #
        # We do this by taking the count in the training matrix per item.
        # vertically stacking these values to get these counts for each user
        # And we then add the interactions, that only occur in the prediction dataset,
        # for prediction users,
        #
        # This gives us per user the accurate count per item,
        # taking into account training data, and only their own history
        # from the prediction dataset.
        item_counts_per_user = (
            np.vstack([train_item_counts for _ in range(X.shape[0])])
            + combined_interactions_only_predict
        )

        # Similarities are computed by matrix multiplication of two interaction matrices
        # the training matrix is scaled by dividing each interaction by
        #   the square root of the number of user interactions.
        # The combined interactions for prediction users is scaled by dividing
        #   by the square root of user interactions
        #   and by the square root of the interactions with the item.
        # fmt:off
        similarities = (
            combined_interactions_selected_users.multiply(
                invert_np_array(np.sqrt(pred_user_interaction_counts))
            ).multiply(
                invert_np_array(np.sqrt(item_counts_per_user))
            )
            @
            self.training_interactions_.multiply(
                invert_np_array(np.sqrt(train_user_counts))
            ).T
        )
        # fmt:on

        similarities.setdiag(0)

        return get_top_K_values(similarities, self.Ku)
